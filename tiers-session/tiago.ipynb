{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On tier representation\n",
    "\n",
    "As with biology, linguistic data and relationships are commonly represented and visualized with one of three fundamental structures: the sequence, for sequential data, the tree, for hierarchical non-sequential data, and the network, for non-hierachical non-sequential data. Typical examples are n-grams, syntax tree, and colexification networks.\n",
    "\n",
    "![ngrams](ngrams.jpg)\n",
    "\n",
    "Ngrams\n",
    "\n",
    "![syntax tree](syntaxtree.jpg)\n",
    "\n",
    "Syntax Tree\n",
    "\n",
    "![colex](colex.jpg)\n",
    "\n",
    "Colexification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential representation is by far the most common one for representing sound sequences, both for easiness of using it, for the example of alphabets, and for the fact that sound sequences are themselves sequential and temporally delimited. As such, most mathematical model of utterances represent words as sequences of elements from a random distribution. It is important to remember that, in this sense, \"random\" does not necessarily means \"without a specific pattern\" or \"unpredictable\" (as the sequence of sounds is usually quite predictable due to the language inventory and to the phonotactics), but just that the sequence is composed of different elements.\n",
    "\n",
    "The most common way of modelling such sequentes is with a Markov model. While there are many different structures and approaches that qualify as \"Markovian\" (such as Markov chains, Hidden Markov Models, etc.), we can simply think of a Markov model simply as a model describing a sequence of possible events (the \"elements\") in which the probability of each event depends only on the state attained in the previous event. For example, in English, if we have no other information about the context of occurence, after a sound /ð/ (represented in writing by \"TH\") we are right to expect the sound /iː/ (\"E\") as the most probable, as the article \"THE\" is the most common word in the language.\n",
    "\n",
    "We can, in fact, easily train a model on such premises. If we consider a context composed only of one sound (a \"monogram\", even though larger, combined, and other alternatives are possible), we can easily come up with a matrix of probabilities of transition from state to state (i.e., from sound to sound) such as the following:\n",
    "\n",
    "|     | aɪ | aʊ    | b    | d    | dʒ   | (...) |ʒ    | θ |\n",
    "|-----|----|-------|------|------|------|-------|------|---|\n",
    "| aɪ | 0.03 | 0.04 | 2.53 | 9.18 | 0.55 |-------| 0.00 | 0.10 |\n",
    "| aʊ | 0.03 | 0.00 | 2.01 | 4.68 | 0.25 |-------| 0.00 | 2.20 |\n",
    "| b | 2.24 | 1.20 | 0.00 | 0.51 | 0.15 |-------| 0.00 | 0.01 |\n",
    "| d | 2.50 | 1.09 | 0.79 | 0.02 | 0.02 |-------| 0.00 | 0.05 |\n",
    "| dʒ | 1.18 | 0.14 | 0.04 | 2.49 | 0.00 |-------| 0.00 | 0.00 |\n",
    "| (...) |\n",
    "| ʒ | 0.55 | 1.11 | 0.00 | 2.21 | 0.00 |-------| 0.00 | 0.00 |\n",
    "| θ | 1.77 | 0.71 | 1.15 | 0.62 | 0.09 |-------| 0.00 | 0.00 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a model such as this is reasonably effective given its simplicity, it fails to support more advanced modelings such as those described above. In terms of patterns of sounds, for example, while the probability distributions it involves could be used as a first step for generalizing observations, there are limits in terms of context and not much can be done without a human intervention in each single stage. In terms of phonetic representation, it cannot fully account for suprasegmental features such as tones, i.e., the contrastive elements that cannot be analyzed as distinct segments but belong to a subgroup of them (sometimes not even following the boundaries of the segments themselves). For pseudo-word generation, not only the context is too limited to capture complex pattern and medium- and long-distance relationships, it also cannot encode information that is above the level of the individual elements, such as if the pseudo-word being generated is a verb or a noun.\n",
    "\n",
    "While some of the problems can be solved or partially remediated with more complex approaches, such as higher order n-grams (i.e., more context), or combined information, we cannot surpass the limit that each element can only encode one item of information.\n",
    "\n",
    "One possible objection to this is that information such as IPA graphemes is by no way atomic: even without considering possible suprasegmental information or more context, a graphame such as /b/ already carries many levels, or \"tiers\", of information, such as its manner of articulation (\"occlusive\"), its place of articulation (\"bilabial\"), its voiceness (\"voiced\"), and so on. In actual words, more information can be obtained by inspection of the word where it is pronuounced, such as the stress of its syllable and its intonation, and even the system where it is pronounced, such as the frequency of such word, its age, the donor language in case of borrowings, and so on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
