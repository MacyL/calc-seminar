{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Borrowing detection and annotation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a narrow sense, borrowing refers to the transfer of words from one language into another one:\n",
    "\n",
    "> Narrowly, the transfer of a word from one language into a second language, as a result\n",
    "> of some kind of contact [...] between speakers of the two. (Trask 2000: 44)\n",
    "\n",
    "In a broader sense, however, borrowing can also refer to the transfer of linguistic feattures among languages:\n",
    "\n",
    "> Broadly, the transfer of linguistic features of any kind from one language to another as\n",
    "> a result of contact. (ebd.)\n",
    "\n",
    "In terms of terminology, it is important to keep in mind that the term \"borrowing\" usually refers to a concrete process. Alternatively, one can also use the word \"transfer\", although we find it less frequently in the literature. Borrowing in a narrower sense would then be called \"lexical transfer\", while the general phenomenon is probably best referred to as \"lexical interference\" (following [Weinreich 1953](:bib:Weinreich1953)).\n",
    "\n",
    "If we assume a bilateral sign model, following Saussure, we can distinguish roughly two cases of lexical transfer, namely the ones where it happens direct (*direct transfer*) and those cases, where the transfer is indirect, i.e., not including the form-part of the linguistic sign, but rather its meaning. For simple words, this can be easily visualized, while we have mixed forms if compounds or more complex signs are involved.\n",
    "\n",
    "\n",
    "> The ways in which one vocabulary can interfere with another are various. Given two\n",
    "> languages, A and B, morphemes may be transferred from A into B, or B-morphemes may\n",
    "> be used in new designative functions on the model of A-morphemes with whose content\n",
    "> they are identified; finally, in the case of compound lexical elements, both processes may\n",
    "> be combined. (Weinreich 1953: 47)\n",
    "\n",
    "We can display those processes based on Weinreich's description (pp. 47-62) in the following image:\n",
    "\n",
    "![img](img/s13_weinreich.png)\n",
    "\n",
    "A further important aspect to be mentioned here is that the process of borrowing does not necessarily end with the transfer of a form or a meaning-structure. After the process, the recipient language usually experiences certain follow-up processes, as the lexical space needs to be re-arranged to accommodate the new word form or conceptual construct. As a result, we can find cases such as *confusion*, *disappearance*, and *specialization*:\n",
    "\n",
    "> Except for loanwords with entirely new content, the transfer or reproduction of foreign\n",
    "> words must affect the existing vocabulary in one of three ways: (1) confusion between\n",
    "> the content of the new and old word; (2) disappearance of the old words; (3) survival of\n",
    "> both the new and old word, with a specialisation in content. (ebd.)\n",
    "\n",
    "From the perspective of the transferred form, we can furthermore distinguish additional processes, namely *nativization*, and its counterpart: *hyper-foreignization* (see [Hock and Joseph 1995: 257f](:bib:Hock1995)). While nativization will adapt the pronunciation of foreign words to the recipient language's basic pronunciation, hypoer-foreignization modifies the foreign sound of borrowings to conserve the foreignness of the borrowed word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Approaches to Borrowing Detection\n",
    "\n",
    "Traditionally, borrowings can be detected using a richer arsenal of different techniques that are rarely formalized and followed up in a straightforward fashion. Among the most important considerations that scholars use as evidence to prove that a word form is borrowed are:\n",
    "\n",
    "1. phonotactic considerations (following the idea that nativization is not an abrupt process, and that borrowed words *sound* different from native words when first introduced to a new language)\n",
    "2. topological considerations (following the idea that a word that is rare in one subgroup but frequent in another one is probably borrowed from the subgroup where it is frequent)\n",
    "3. semantic considerations (following the idea that a word with a specialized meaning range that provides further evidence for being borrowed is judged to be more likely to be borrowed if it represents a restricted or very specialized, e.g., technical, meaning)\n",
    "4. irregular sound correspondences (following the idea that irregular sound correspondences may reflect transfer instead of inheritance in potential cognate words)\n",
    "\n",
    "The last point is currently regarded as the most important pieces of evidence for borrowings. If sound correspnodences between two languages show an unexpected behavior, scholars identify *layers* of borrowing. A simple example for this *stratification* approach is the match of *d* in German and English related words. Usually, we would expect English words to show a *th* if the German words starts with a *d*, but we have a layer of many borrowings in German, where we have *d* in both languages:\n",
    "\n",
    "No. | German | English | Middle High German \n",
    "--- | --- | --- | ---\n",
    "1 | Dach | thatch | dah\n",
    "2 | Daumen | thumb | dūm\n",
    "3 | Degen | thane | degan\n",
    "4 | Ding | thing | ding\n",
    "5 | drei | three | drī\n",
    "6 | Durst | thirst | durst\n",
    "7 | denken | think | denken\n",
    "8 | Dieb | thief | diob\n",
    "9 | dreschen | thresh | dreskan\n",
    "10 | Drossel | throat | drozze\n",
    "11 | Dill | dill | tilli\n",
    "12 | dumm | dumb | tumb\n",
    "13 | Damm | dam | tam\n",
    "14 | Dunst | dunst | tunst\n",
    "15 | Dollar | dollar | -\n",
    "\n",
    "Not all of these are borrowings from English to German, but they can also come from Low German varities. The other methods are less frequently used, at least in the classical literature. The phonotactic method is probably considered to be too shallow in time depth, and the topological considerations suffer from the fact that words can look like borrowings from their distribution in tree topologies although they are not borrowed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic Borrowing Detection\n",
    "\n",
    "This method, which has been described in a couple of papers and implemented as part of LingPy under the name \"MLN method\" ([Nelson-Sathi et al. 2011](:bib:NelsonSathi2011), [List et al. 2014](:bib:List2014f)) takes tree topology as a proxy for the detection of borrowed words. The main idea is that words which show a strange distribution across a given *reference tree* are likely to be borrowed. While this assumption does not hold in all cases, it seems to be useful enough to get at least a rough impression of how much reticulation can be found in a given dataset. \n",
    "\n",
    "As an example, consider the distribution of words for \"mountain\" in German and Romance languages in the following table:\n",
    "\n",
    "![img](img/s13-fig1.png)\n",
    "\n",
    "If we seek to explain this distribution, we can evoke different evolutionary scenarios of how the words corresponding to pattern A developed across the tree, as shown in the next figure (scenarios in B and C):\n",
    "\n",
    "![img](img/s13-fig2.png)\n",
    "\n",
    "If these methods are applied to larger datasets, a phylogenetic tree can be inferred in which edges between nodes are drawn that visualize where the conflicts can be found. Often these edges may coincide with lateral transfer events in language history, but they are at times also simplifying, as we cannot tell the direction, or the concrete donor, as in our case for *mountain*, where the donor could be any of the Romance languages (not only in our sample, but even outside). \n",
    "\n",
    "Nevertheless, networks drawn in this fashion may yield initial interesting insides into the data of a certain dataset.\n",
    "\n",
    "![img](img/s13-fig3.png)\n",
    "\n",
    "In a similar fashion, we can plot the networks to a map, where we can often find much more detailed accounts, and also an easy way to verify if a given \"borrowing event\" makes sense in the light of the data.\n",
    "\n",
    "![img](img/s13-fig4.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language-Internal Borrowing Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing MLNs with LingPy\n",
    "\n",
    "In the following, we quickly demonstrate, how MLNs can be computed in LingPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from lingpy.compare.phylogeny import PhyBo\n",
    "\n",
    "phy = PhyBo('../data/S10-BAI.tsv', tree_calc='upgma')\n",
    "phy.analyze()\n",
    "phy.plot_MLN(fileformat='png', filename='img/s13-fig5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assumes a lot of data being already passed to the algorithm, especially cognate sets and cognate sets.\n",
    "\n",
    "![img](img/s13-fig5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection with stochastic methods\n",
    "\n",
    "Another methods based purely in language-internal information for loan word detection is considering all words in a language as \"sequences\" of states (be it either letters or phonemes) and computing a model that allows to score the probability (or, more properly, the \"relative likelihood\") of a sequence given what has already been observed. The advantage of this method is that, while uninformed and as expected limited in its accuracy, it allows to identify possible loanwords not only with multiple language wordlists, but also within a single language.\n",
    "\n",
    "We rely in the aforementioned idea \n",
    "\n",
    "### Stochastic processes and Markov chains\n",
    "\n",
    "This automatic detection also allows us to cover stochastic (i.e., involving chance or probability) process in the form of the most common ones, Markov chains. A Markov chain is the simples of all Markov models, used to describe the state of a system in which a random variable changes through time. While many other factors and variables might be at play, especially in linguistics with so called \"long-range dependencies\", a Markov chain assumes that the distribution of the states of a variable depends only upon its previous state. In the simple example below, we model the weather of a particular city as having only three possible states (\"sunny\", \"partly cloudy\", and \"rainy\") and we attribute the probability for a transition from one state to the other; for example, it assumes that a sunny day is followed 50% of the time by another sunny day, 40% by a cloudy day, and only 10% of the time by a rainy day. Probabilities can be shown both as a directed graph or as matrix.\n",
    "\n",
    "![Markov Chain](img/markovchain.png)\n",
    "\n",
    "|        | Start / First day |Sunny   | Cloudy | Rainy |\n",
    "|--------|-------------------|--------|--------|-------|\n",
    "|  Sunny | 0.333             | 0.5    | 0.4    | 0.1   |\n",
    "| Cloudy | 0.333             | 0.4    | 0.5    | 0.1   |\n",
    "| Rainy  | 0.333             | 0.2    | 0.2    | 0.6   |\n",
    "\n",
    "Given this model, we can calculate probabilities for a given set of observations. For example, an observation of three sunny days (SSS) has an overall probability of 0.333 x 0.5 x 0.5 = 0.083, with 33% of the probability of beginning with a sunny day and 50% of probability of moving from a sunny day to a second sunny day. Likewise, an observation of a sunny day followed by a rainy day followed by a cloudy day (SRC) has a lower overall probability of 0.333 x 0.1 x 0.2 = 0.006 (the same 33% of starting with a sunny day, followed by 10% of probability of moving from sunny to rainy, followed by 20% of probability of moving from rainy to cloudy).\n",
    "\n",
    "### Markov models of languages\n",
    "\n",
    "Markov models usually work upon collections of words, but we can perfectly use them with sounds (i.e., IPA glyphs). We will describe here the results of a paper we submitted, the code for which is being integrated in LingPy and should be available in the upcoming 2.7 release. We rely in the hypothesis that a word relative likelihood can in some cases be used as a proxy for *nativization*, i.e., the process by which words borrowed from a donor language are successively incorporated into the system of the recipient language by modifying their phonology and morphology. The pace by which words are integrated during language history can vary substantially, depending on various factors, such as the similarity of the phonological systems of donor and recipient language, the degree of bilingualism among the speakers, or the amount of words that have already been borrowed at the point of initial adoption. Nativization often also proceeds along different stages: after initial (often regular) transformations that guarantee that the borrowed word is pronounceable in the recipient language, successive changes in the phonological and morphological system of the recipient language successively adapt the originally foreign word to sound more and more like a native word, until, at some point, speakers are no longer aware that a word has been borrowed at all.\n",
    "\n",
    "As an example for the nativization process, consider the recent adoption of Italian *spaghetti* [spagɛti] as German *Spaghetti* [ʃpagɛtiː]. While the initial consonant cluster has been modified to account for current phonotactic rules of the phonological system of German, according to which [sp] is rendered as [ʃp], the word remains foreign for the German phonological system, since German words are rarely trisyllabic (unless they are clearly compounded), and usually do not end in long unstressed vowels. A word like German Pfeffer [pfɛfɐ], on\n",
    "the other hand, is perfectly integrated in the German system and not recognizable as a loanword, although\n",
    "it was originally borrowed from either Latin *piper* [pipər] or Greek *péperi* [ˈpɛpəri]. The borrowing, however, occurred rather early, before the 8th century AD, and especially the change of [p] to [pf] in initial position and to [f] in intervocalic position (due to the second consonant shift before the formation of Old High German) contributes to mask the foreign origin of the word.\n",
    "\n",
    "#### Initial Training and Testing\n",
    "\n",
    "In order to infer optimal parameters for our approach and to make sure our basic assumptions hold, we trained an initial estimator on an English electronic dictionary. In the table below we present the log-probabilities\n",
    "computed by this estimator, when using different smoothing methods, for three different strings: one included in the training set (\"world\"), one not included but composed of observed states (\"dkx\"), and one not included and composed of unobserved states (\"可能性”). The ML, WB, and SGT columns refer to different smoothing methods, i.e., ways of estimating the probability that some states (some sequences of letters) exist but have not been observed. We can discuss them if you want.\n",
    "\n",
    "| Word   | ML      | WB      | SGT     |\n",
    "|--------|---------|---------|---------|\n",
    "|  world | -56.70  | -56.78  | -56.68  |\n",
    "| dkx    | -122.54 | -74.40  | -72.15  |\n",
    "| 可能性  | -322.35 | -155.63 | -213.29 |\n",
    "\n",
    "As a more interesting result, the table below presents the log-probabilities computed for a sample of random words, along with their corresponding percentile rank. The probabilities for the words were confirmed to be normally distributed.\n",
    "\n",
    "| Word         | Probability | Percentile Rank |\n",
    "|--------------|-------------|-----------------|\n",
    "| Harrisburg   | -99.95      | 0.1045          |\n",
    "| quarantining | -95.21      | 0.1496          |\n",
    "| derivative   | -82.09      | 0.3411          |\n",
    "| mission's    | -65.33      | 0.6809          |\n",
    "| brawl        | -55.82      | 0.8607          |\n",
    "\n",
    "The same English estimator was tested with entries from the Italian and Polish electronic dictionaries. The probabilities computed for both these sets also show normal distributions, with the mean probability for Italian higher than that for Polish. This is due to the number of shared words in the English and the Italian data (especially proper names and musical terms). Nonetheless, the distributions are clearly di erentiated, supporting our hypothesis that the probability of sequence of states can be used as a proxy for the probability of a word being related to a given set of words.\n",
    "\n",
    "![english italian polish](img/english-italian-polish.png)\n",
    "\n",
    "#### Loanword detection\n",
    "\n",
    "We decided to use the cross-linguistic data of the World Loanword project (WOLD, Haspelmath and Tadmor, 2009) as our primary data for testing and training. WOLD provides translations of a base set of 1480 concepts into 41 di erent languages from 27 different language families, which can be obtained at the [WOLD website](http://wold.clld.org/); we expanded with some more datasets.\n",
    "\n",
    "When using lists of native words and loanwords, i.e., separate lists for each language where the status of each  word had been previously annotated by experts, the results were still judged satisfying for most datasets, especially when using probabilities over ranks, with a mean F1 score for the loanword categorization test of 0.87 across all datasets (minimum of 0.58 for Old High German, and a maximum of 0.94 for Selice Romani, both from the WOLD dataset).\n",
    "\n",
    "When using only lists of native words (i.e., excluding words known to have been borrowed from the training), we found marked differences in performance across the datasets in terms of the distribution of ranks for words annotated as loans in the sources, with potentially acceptable results when considering that the median rank for native words is, by definition, 0.5 (e.g., in Bezhta: median of 0.26, μ = 0.31, Q 3 = 0.43) and unreliable results (e.g., in Mandarin Chinese: median of 0.84, μ = 0.80, Q 3 = 0.92, all from WOLD). An investigation of the reasons for such difference clarified that multiple issues are involved, including dataset features (type of transcription and percentage of loans), and language properties (phonology/phonotactics and loanword adoption tendency). This confronts us with a circular problem: if loanword identi cation is hard for experts, this will result in datasets of low borrowing percentages, which are themselves unable to inform a method for automatic loanword identification.\n",
    "\n",
    "The most interesting results are probably those of the most likely scenario, without annotated lists: a researcher would only have a list of words from a language, without any information on potentional loan status not lists of words of similar or neighouring languages.\n",
    "\n",
    "We found a mean percentile rank for loanwords across languages of 0.3024 (minimum of 0.1324 for Manange and maximum of 0.5590 for White Hmong, both from the WOLD dataset). Our analysis suggests that, while the probabilities for loanwords computed in this uninformed scenario are, as expected, generally higher than those of an informed one, and thus less reliable, no major overall decline in performance is experienced when using percentile ranks, even when considering that the median rank of native words cannot be assumed to be equal to 0.5 as in the previous case.\n",
    "\n",
    "In case of datasets with information on time of loanword adoption, it was also possible to confirm that, while the variation is too large to infer the age of adoption from a word score, the relative likelihood is correlated with the age of adoption, as in the example below for German.\n",
    "\n",
    "\n",
    "| Adoption time | Mean Log-Probability | Mean Percentile Score |\n",
    "|---------------|----------------------|-----------------------|\n",
    "|  8. c.        | -54.22               | 0.4515                |\n",
    "| 9. c.         | -53.52               | 0.4833                |\n",
    "| 10. c.        | -52.74               | 0.5095                |\n",
    "| 11. c.        | -56.31               | 0.4208                |\n",
    "| 12. c.        | -59.38               | 0.4048                |\n",
    "| 13. c.        | -60.32               | 0.3715                |\n",
    "| 14. c.        | -62.52               | 0.3422                |\n",
    "| 15. c.        | -66.14               | 0.2780                |\n",
    "| 16. c.        | -70.73               | 0.2309                |\n",
    "| 17. c.        | -67.53               | 0.2591                |\n",
    "| 18. c.        | -71.28               | 0.2224                |\n",
    "| 19. c.        | -68.61               | 0.2656                |\n",
    "| 20. c.        | -66.32               | 0.3140                |\n",
    "\n",
    "#### Before finishing\n",
    "\n",
    "Let's experiment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
