{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring, Modeling, and Analysing Alignments (Johann-Mattis List and Mei-Shin Wu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Sequence Comparison\n",
    "\n",
    "Sequence comparison refers to techniques by which sequential structures can be compared formally and automatically. In order to understand the basic principles (algorithms and models) underlying sequence comparison in historical linguistics, it is important to develop a general idea of *sequences* first, independent of the specific aspects or implications of sequences in historical linguistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Sequences\n",
    "\n",
    "Many structures we are dealing with – be it in daily life or in science – can\n",
    "be represented as sequences. The bird songs which awake us in the morning\n",
    "are sequences of sound waves, the movies we watch are sequences of pictures,\n",
    "and the meals we cook are created by a sequence of instructions received from\n",
    "a recipe book.\n",
    "\n",
    "![image](img/s7-sequences.png)\n",
    "\n",
    "When dealing with sequences, we need to keep some general questions in mind, which are crucial for an understanding of sequences. For example, not all sequences are discrete, but many sequences in our real life are continuous. If we pronounce a word, or listen to a piece of music, it is not always straightforward to *segment* the sequence we perceive into distinct units. Instead, we have too deal with continuous entities, and we use the process of *segmentation* as a model-theoretic idealization that facilitates our research. We should however, not forget, that all segmentation is in fact an idealization.\n",
    "\n",
    "Keeping this in mind, we can define sequences as follows:\n",
    "\n",
    "> Given an alphabet (a non-empty finite set, whose elements are\n",
    "> called characters), a sequence is an ordered list of characters drawn from the alphabet. The elements of sequences are called segments. The length of a sequence\n",
    "> is the number of its segments, and the cardinality of a sequence is the number its\n",
    "> unique segments. (cf. [Böckenbauer and Bongartz 2003](:bib:Boeckenbauer2003): 30f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sequence Comparison\n",
    "\n",
    "Comparing sequences may turn out to be a rather simple task. This is especially the case when it is known in advance, which segments of the sequences\n",
    "correspond to each other. Consider, e.g., two strings of colored beads which\n",
    "are of the same length: Comparing these sequences, we simply have to line\n",
    "them up and check whether the same colors appear in the same positions, as\n",
    "illustrated in below figure. It is also very easy to quantify the difference between the two strings by simply counting the number of positions in which\n",
    "both strings differ, which yields 2 for the two strings in the example, since\n",
    "the strings differ in positions three and four. The result of such a count is a \n",
    "distance in the strict mathematical sense, also known as Hamming distance,\n",
    "which was first introduced by R. W. Hamming (1915 – 1998) in a paper from\n",
    "1950 ([Hamming 1950](:bib:Hamming1950)).\n",
    "\n",
    "![image](img/s7-seqcom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a comparison of the structure of sequences, however, the Hamming distance is surely not enough. Instead, we need to change our perspective, accepting that not all sequences have the same length. A very common perspective for the purpose of sequence comparison is the *edit perspective*. According to this perspective, we compare the structure of two sequences by imagining how we could convert one of them in the other. Here, we can define different basic operations, such as\n",
    "\n",
    "* deletion (delete a segment from a given sequence)\n",
    "* insertion (insert a segment into a given sequence)\n",
    "* substitution (replace a segment by another one)\n",
    "* transposition (have to segments change places)\n",
    "* expansion (make *n* out of one segment)\n",
    "* compression (make one out of *n* segments)\n",
    "\n",
    "The following graphic illustrates, how we can convert one sequence into another.\n",
    "\n",
    "![images](img/s7-edit.png)\n",
    "\n",
    "More generally, we can model these operations in form of *correspondences* (matches) between segments. For reasons of simplicity, let's assume that we deal only with the first three edit operations. The following graphic shows, which correspondences relate to them.\n",
    "\n",
    "![images](img/s7-matches.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Alignment Analyses\n",
    "\n",
    "Alignment analyses are the most general way to model sequence differences. We can describe alignment analyses of two sequences in the following way:\n",
    "\n",
    "> An alignment is a matrix with two rows in which two sequences\n",
    "*s* and *t* are ordered in such a way that all proper matches appear in the same\n",
    "column. Empty matches are filled with gap symbols ([Kruskal 1983](:bib:Kruskal1983): 211).\n",
    "\n",
    "With the help of alignment analyses 3 all different kinds of sequences can be\n",
    "compared, regardless of where they occur or what the purpose of the comparison is. Thus, when trying to detect plagiarism in scientific work, alignment\n",
    "analyses can throw light on the differences between the original text and the\n",
    "plagiary (see Example (1) in the figure below). In molecular biology, the alignment of protein and DNA sequences is a very common method and the basis\n",
    "of phylogenetic reconstruction (see Example (2) in below figure), and in type setting programs and search engines, sequence alignments can be used to detect spelling errors (see Example (3)).\n",
    "\n",
    "![img](img/s7-alignments.png)\n",
    "\n",
    "A more general definition of alignment analyses for *n* (as opposed to two) sequences can be given as follows:\n",
    "\n",
    "> An alignment of *n* (*n* > 1) sequences is an *n*-row matrix in which\n",
    "all sequences are aranged in such a way that all matching segments occur in the\n",
    "same column, while empty cells, resulting from empty matches, are filled with\n",
    "gap symbols. The number of columns of an alignment is called its length or its\n",
    "width, and the number of rows is called its height. (cf. [Gusfield 1997](:bib:Gusfield1997): 216)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Phonetic Alignments\n",
    "\n",
    "Although alignment analyses are a very general way to compare sequences, they are not\n",
    "frequently being used in historical linguistics. Obviously, historical linguists align words in\n",
    "their heads, because without alignments, we could never identify regular sound correspon-\n",
    "dences, but most of the time, these comparisons are carried out implicitly, and they are\n",
    "rarely visualized. In addition, we often have problems when comparing words, since not all\n",
    "elements in historically related words are necessarily alignable. This is illustrated in the following illustration, where two different alignments are contrasted with each other.\n",
    "\n",
    "![img](img/s7-iexample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Types of Sound Change\n",
    "\n",
    "There is a long tradition of classifying specific sound changes into different types in historical linguistics. Unfortunately, the terminology is not very neat, ranging from very specific\n",
    "terms up to very abstract ones. We thus find terms like “rhotacism” ([Trask 2000](:bib:Trask2000): 288), which\n",
    "refers to the change of [s] to [r], but also terms like lenition, which is a type of change “in\n",
    "which a segment becomes less consonant-like than previously” (ebd.: 190). Some terms\n",
    "are furthermore rather “explanative” than “descriptive” because they also denote a reason\n",
    "why a change happens, Thus, assimilation is often not only described as “[a] change in\n",
    "which one sound becomes more similar to another”, but it is instead also emphasized that\n",
    "this happens “through the influence of a neighboring, usually adjacent, sound” ([Campbell\n",
    "und Mixco 2007](:bib:Campbell2007): 16).\n",
    "\n",
    "The following table lists five more or less frequent types of sound change, by simply\n",
    "pointing to the relation between the source and the target, which serves as the sole criterion\n",
    "for the classification:\n",
    "\n",
    "![image](img/s7-types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sound Classes\n",
    "\n",
    "We need to keep in mind that substantial differences between sounds (like between [p] and\n",
    "[b] or [f]) do not necessarily allow us to conclude that the words are not related, as sound\n",
    "change often follows certain general preferences. On the other hand, surface similarity\n",
    "between sounds does not prove anything in historical linguistics, unless we can show that\n",
    "this similarity is also regular (in terms of recurrent sound correspondences). Nevertheless,\n",
    "if we want to find cognate words, or get an idea on how to align two words we have not\n",
    "seen before, it is useful to turn to surface similarities to guide our first analysis. We thus\n",
    "need a heuristics that enables us to search for probably corresponding elements.\n",
    "\n",
    "To account for this, we can make use of the concept of sound classes which was first\n",
    "proposed by [Dolgopolsky (1964)](:bib:Dolgopolsky1964). The basic idea is that sound which often occur in corre-\n",
    "spondence relation across the languages of the world can be divided in classes such that\n",
    "“phonetic correspondences inside a ,type’ are more regular than those between different\n",
    ",types’” (ebd.: 35).\n",
    "\n",
    "![img](img/s7-classes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Morphemes and Secondary Structures\n",
    "\n",
    "Words can be segmented into sounds, but they can also be secondarily segmented, for\n",
    "example into syllables or morphemes. The morpheme structure of words plays a crucial\n",
    "role in phonetic alignment, since it governs the way we compare words.\n",
    "\n",
    "The table below gives an example for the differences between a naive primary alignment\n",
    "and an informed secondary alignment While the primary alignment infers a wrong corre-\n",
    "spondence between final [t] and initial [th], the secondary alignment correctly matches\n",
    "only the first morpheme ʐʅ⁵¹ “sun” of the Běijīng word and separates the suffix tʰou¹ “head\n",
    "(suffix)”.\n",
    "\n",
    "![img](img/s7-morphs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Alignability\n",
    "\n",
    "Not all aspects of language are completely sequential. We also find many hierarchical\n",
    "aspects. Word formation, for example, is often hierarchic, resembling syntax. If we want\n",
    "to compare sound sequences which have an underlying hierarchical structure, a normal\n",
    "alignment can only be used if the underlying structures are similar enough. If this is not the\n",
    "case, an alignment of entire words does not make sense. Instead, we need to identify and\n",
    "annotate those elements which are alignable. A more proper rendering of the structure of\n",
    "words for “sun” for example, can be found here:\n",
    "\n",
    "![img](img/s7-alignables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Computing and Analysing Alignments with LingPy and EDICTOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Computing Alignments with LingPy\n",
    "\n",
    "\n",
    "LingPy (Python Library for Historical Linguistics, http:lingpy.org) is a suite of open source Python modules to analyze large scale linguistic data. It provides sequence comparison, distance analyses, computational cognate judgement. visualization, and more. \n",
    "\n",
    "#### 3.1.1 Examples\n",
    "\n",
    "We use the following toy data file (please find ```S07_edictor.tsv``` in data folder)\n",
    "\n",
    "|   ID | DOCULECT   | CONCEPT   | IPA     | TOKENS      |   COGID |\n",
    "|-----:|:-----------|:----------|:--------|:------------|--------:|\n",
    "|    1 | Beijing    | ASH       | xuei⁵⁵  | x u e i ⁵⁵  |       1 |\n",
    "|    2 | Guangzhou  | ASH       | fui⁵³   | f u i ⁵³    |       1 |\n",
    "|    3 | Meixian    | ASH       | foi³³   | f o i ³³    |       1 |\n",
    "|    4 | Jinuo      | ASH       | a³³mɐ³³ | a ³³ m ɐ ³³ |       2 |\n",
    "|    5 | Lahu       | ASH       | mə³³e³³ | m ə ³³ e ³³ |       2 |\n",
    "\n",
    "\n",
    "In the following, we want to test the behavior of the following functions in LingPy when using parts of our toy data.\n",
    "\n",
    "* edit distance (```edit_dist```): compute the edit distance between two strings\n",
    "* Needleman-Wunsch (```nw_align```): compute a classical alignment between two strings\n",
    "* Smith-Waterman (```sw_align```): compute the classical local alignment between two strings\n",
    "* SCA alignment (```Pairwise```): compute alignments with help of the SCA algorithm ([List 2012](:bib:List2012b))\n",
    "* multiple alignment (```Multiple```): compute multiple alignments with LingPy\n",
    "* aligning data in text-file (```Alignments```): compute multiple alignments from data from a text file\n",
    "\n",
    "Let's first load our data and have a quick look at it (using ```tabulate``` to print nice tables):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   ID | DOCULECT   | CONCEPT   | IPA     | TOKENS      |   COGID |\n",
      "|-----:|:-----------|:----------|:--------|:------------|--------:|\n",
      "|    1 | Beijing    | ASH       | xuei⁵⁵  | x u e i ⁵⁵  |       1 |\n",
      "|    2 | Guangzhou  | ASH       | fui⁵³   | f u i ⁵³    |       1 |\n",
      "|    3 | Meixian    | ASH       | foi³³   | f o i ³³    |       1 |\n",
      "|    4 | Junuo      | ASH       | a³³mɐ³³ | a ³³ m ɐ ³³ |       2 |\n",
      "|    5 | Lahu       | ASH       | mə³³e³³ | m ə ³³ e ³³ |       2 |\n"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from tabulate import tabulate\n",
    "data = csv2list('../data/S07_toy-data.tsv')\n",
    "print(tabulate(data[1:], headers=data[0], tablefmt='pipe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start aligning the words in different ways. We start from the edit distance, which does internally align the data, but does not report the alignment to us. Instead, it returns the number of edit operations, which are needed in order to convert one string into another. Let us start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_dist('mama', 'papa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the raw edit distance, as we need two substitutions to convert one string into the other. However, we can also compute the normalized edit distance, which further divides the raw edit distance by the length of the longer string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_dist('mama', 'papa', normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even allow for a specific restriction that would disallow to replace vowels by consonants. We can test this with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n"
     ]
    }
   ],
   "source": [
    "dA = edit_dist('mama', 'mpmp') # normal edit distance\n",
    "dB = edit_dist('mama', 'mpmp', restriction='cv') # restriction: consonants cannot be replaced by vowels\n",
    "print(dA, dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see: the restriction forces the algorithm to replace the \"a\" by a \"p\" with help of two edit operations (one deletion, and one insertion).\n",
    "\n",
    "Let us now look at alignments, and how they are handled in LingPy. We start with the simple Needleman-Wunsch algorithm ([Needleman and Wunsch 1970](:bib:Needleman1970)), and the same strings. LingPy returns three values for this function, the first sequence, the second sequence, and the similarity score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\ta\tm\ta\n",
      "p\ta\tp\ta\n",
      "0.00\n"
     ]
    }
   ],
   "source": [
    "sA, sB, sim = nw_align('mama', 'papa')\n",
    "print('\\t'.join(sA))\n",
    "print('\\t'.join(sB))\n",
    "print('{0:.2f}'.format(sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder, why we have \"0\" as score here. But this is because the Needleman-Wunsch algorithm sums up points for gains and points for mismatches, with mismatches being scored as -1. So in total, we have a score of 2 times 1 plus 2 times -1, which is \"0\".\n",
    "\n",
    "Let us now look at some slightly more complex alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\ta\t-\t-\t-\tu\ts\n",
      "p\ta\tk\th\ta\tu\t-\n",
      "-3.00\n"
     ]
    }
   ],
   "source": [
    "sA, sB, sim = nw_align(['m', 'a', 'u', 's'], ['p', 'a', 'k', 'h', 'a', 'u'])\n",
    "print('{0}\\n{1}\\n{2:.2f}'.format('\\t'.join(sA), '\\t'.join(sB), sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's come back to our toy data! \n",
    "First of all, the edit distance between all language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing\tGuangzhou\txuei⁵⁵\tfui⁵³\t3\t0.5\t3\n",
      "Beijing\tJunuo\txuei⁵⁵\ta³³mɐ³³\t7\t1.0\t9\n",
      "Beijing\tLahu\txuei⁵⁵\tmə³³e³³\t7\t1.0\t7\n",
      "Beijing\tMeixian\txuei⁵⁵\tfoi³³\t5\t0.8333333333333334\t5\n",
      "Guangzhou\tJunuo\tfui⁵³\ta³³mɐ³³\t6\t0.8571428571428571\t7\n",
      "Guangzhou\tLahu\tfui⁵³\tmə³³e³³\t6\t0.8571428571428571\t6\n",
      "Guangzhou\tMeixian\tfui⁵³\tfoi³³\t2\t0.4\t2\n",
      "Junuo\tLahu\ta³³mɐ³³\tmə³³e³³\t4\t0.5714285714285714\t4\n",
      "Junuo\tMeixian\ta³³mɐ³³\tfoi³³\t5\t0.7142857142857143\t6\n",
      "Lahu\tMeixian\tmə³³e³³\tfoi³³\t5\t0.7142857142857143\t5\n"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from itertools import combinations\n",
    "# wordlist can accept tsv file or python dictionary.\n",
    "wl=Wordlist('../data/S07_toy-data.tsv')\n",
    "pairs=combinations(wl.doculect,2)\n",
    "for c in pairs:\n",
    "    seqA=wl.get_list(doculect=c[0],entry='ipa')[0]\n",
    "    seqB=wl.get_list(doculect=c[1],entry='ipa')[0]\n",
    "    d=edit_dist(seqA,seqB)\n",
    "    dnor=edit_dist(seqA,seqB, normalized=True)\n",
    "    dres=edit_dist(seqA,seqB, restriction='cv')\n",
    "    print(c[0],c[1],seqA,seqB,d,dnor,dres, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out **Needleman-Wunsch** alignment between all language pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing Guangzhou\n",
      "x\tu\te\ti\t⁵\t⁵\n",
      "f\tu\t-\ti\t⁵\t³\n",
      "0.00\n",
      "\n",
      "Beijing Junuo\n",
      "x\tu\te\ti\t⁵\t⁵\t-\n",
      "a\t³\t³\tm\tɐ\t³\t³\n",
      "-7.00\n",
      "\n",
      "Beijing Lahu\n",
      "x\tu\t-\t-\te\ti\t⁵\t⁵\n",
      "m\tə\t³\t³\te\t-\t³\t³\n",
      "-6.00\n",
      "\n",
      "Beijing Meixian\n",
      "x\tu\te\ti\t⁵\t⁵\n",
      "-\tf\to\ti\t³\t³\n",
      "-4.00\n",
      "\n",
      "Guangzhou Junuo\n",
      "f\tu\ti\t⁵\t-\t³\t-\n",
      "a\t³\t³\tm\tɐ\t³\t³\n",
      "-5.00\n",
      "\n",
      "Guangzhou Lahu\n",
      "f\tu\ti\t⁵\t-\t³\t-\n",
      "m\tə\t³\t³\te\t³\t³\n",
      "-5.00\n",
      "\n",
      "Guangzhou Meixian\n",
      "f\tu\ti\t⁵\t³\n",
      "f\to\ti\t³\t³\n",
      "1.00\n",
      "\n",
      "Junuo Lahu\n",
      "a\t-\t³\t³\tm\tɐ\t³\t³\n",
      "m\tə\t³\t³\t-\te\t³\t³\n",
      "0.00\n",
      "\n",
      "Junuo Meixian\n",
      "a\t³\t³\tm\tɐ\t³\t³\n",
      "-\t-\tf\to\ti\t³\t³\n",
      "-3.00\n",
      "\n",
      "Lahu Meixian\n",
      "m\tə\t³\t³\te\t³\t³\n",
      "-\t-\tf\to\ti\t³\t³\n",
      "-3.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from itertools import combinations\n",
    "wl=Wordlist('../data/S07_toy-data.tsv')\n",
    "pairs=combinations(wl.doculect,2)\n",
    "for c in pairs:\n",
    "    seqA=wl.get_list(doculect=c[0],entry='ipa')[0]\n",
    "    seqB=wl.get_list(doculect=c[1],entry='ipa')[0]\n",
    "    tA, tB, d=nw_align(seqA,seqB)\n",
    "    print(c[0],c[1])\n",
    "    print('\\t'.join(tA))\n",
    "    print('\\t'.join(tB))\n",
    "    print('{0:.2f}\\n'.format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about **Smith-Waterman** alignment? It is an algorithm emphasizes on the local alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing Guangzhou\n",
      "(['x'], ['u', 'e', 'i', '⁵'], ['⁵'])\n",
      "(['f'], ['u', '-', 'i', '⁵'], ['³'])\n",
      "2.0\n",
      "\n",
      "Beijing Junuo\n",
      "(['x', 'u', 'e', 'i', '⁵', '⁵'], [], [])\n",
      "(['a', '³', '³', 'm', 'ɐ', '³', '³'], [], [])\n",
      "0\n",
      "\n",
      "Beijing Lahu\n",
      "(['x', 'u'], ['e'], ['i', '⁵', '⁵'])\n",
      "(['m', 'ə', '³', '³'], ['e'], ['³', '³'])\n",
      "1.0\n",
      "\n",
      "Beijing Meixian\n",
      "(['x', 'u', 'e'], ['i'], ['⁵', '⁵'])\n",
      "(['f', 'o'], ['i'], ['³', '³'])\n",
      "1.0\n",
      "\n",
      "Guangzhou Junuo\n",
      "(['f', 'u', 'i', '⁵'], ['³'], [])\n",
      "(['a', '³', '³', 'm', 'ɐ', '³'], ['³'], [])\n",
      "1.0\n",
      "\n",
      "Guangzhou Lahu\n",
      "(['f', 'u', 'i', '⁵'], ['³'], [])\n",
      "(['m', 'ə', '³', '³', 'e', '³'], ['³'], [])\n",
      "1.0\n",
      "\n",
      "Guangzhou Meixian\n",
      "([], ['f', 'u', 'i', '⁵', '³'], [])\n",
      "([], ['f', 'o', 'i', '³', '³'], [])\n",
      "1.0\n",
      "\n",
      "Junuo Lahu\n",
      "(['a'], ['³', '³', 'm', 'ɐ', '³', '³'], [])\n",
      "(['m', 'ə'], ['³', '³', '-', 'e', '³', '³'], [])\n",
      "2.0\n",
      "\n",
      "Junuo Meixian\n",
      "(['a', '³', '³', 'm', 'ɐ'], ['³', '³'], [])\n",
      "(['f', 'o', 'i'], ['³', '³'], [])\n",
      "2.0\n",
      "\n",
      "Lahu Meixian\n",
      "(['m', 'ə', '³', '³', 'e'], ['³', '³'], [])\n",
      "(['f', 'o', 'i'], ['³', '³'], [])\n",
      "2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from itertools import combinations\n",
    "wl=Wordlist('../data/S07_toy-data.tsv')\n",
    "pairs=combinations(wl.doculect,2)\n",
    "for c in pairs:\n",
    "    seqA=wl.get_list(doculect=c[0],entry='ipa')[0]\n",
    "    seqB=wl.get_list(doculect=c[1],entry='ipa')[0]\n",
    "    d=sw_align(seqA,seqB)\n",
    "    print(c[0],c[1])\n",
    "    print('{0}\\n{1}\\n{2}\\n'.format(d[0],d[1],d[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminders**\n",
    "- Apart from the algorithms present above, LingPy offers many other alignment methods. \n",
    "- LingPy is very flexible, there are many ways to achieve the same goals.\n",
    "\n",
    "**Pairwise v.s. multiple alignment**\n",
    "Aligning between two languages is called pairwise alignment. On the other hand, aligning across several languages is called multiple alignment. (+++some benefits about using multiple alignment instead of pairwise alignment? +++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing ['G', 'Y', 'E', 'I', '5']\n",
      "Guangzhou ['B', 'Y', 'I', '3']\n",
      "Junuo ['A', '4', 'M', 'E', '4']\n",
      "Lahu ['M', 'E', '4', 'E', '4']\n",
      "Meixian ['B', 'U', 'I', '4']\n"
     ]
    }
   ],
   "source": [
    "# convert to SCA model\n",
    "from lingpy import *\n",
    "wl=Wordlist('../data/S07_toy-data.tsv')\n",
    "for d, w in zip(wl.doculect, wl.ipa[0]):\n",
    "    wt=ipa2tokens(w, merge_vowels=False)\n",
    "    print(d, tokens2class(wt, model='sca'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++Please help me with the Pairwise alignment in SCA model, I wanted to try SCA() function+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing Guangzhou\n",
      "x\tu\te\ti\t⁵⁵\n",
      "f\tu\t-\ti\t⁵³\n",
      "11.3\n",
      "\n",
      "Beijing Junuo\n",
      "-\t-\tx\tu\te\ti\t⁵⁵\n",
      "a\t³³\tm\t-\t-\tɐ\t³³\n",
      "2.220446049250313e-16\n",
      "\n",
      "Beijing Lahu\n",
      "-\t-\t-\tx\tu\te\ti\t⁵⁵\n",
      "m\tə\t³³\t-\t-\t-\te\t³³\n",
      "-0.9000000000000001\n",
      "\n",
      "Beijing Meixian\n",
      "x\tu\te\ti\t⁵⁵\n",
      "f\to\t-\ti\t³³\n",
      "10.0\n",
      "\n",
      "Guangzhou Junuo\n",
      "-\t-\tf\tu\ti\t⁵³\n",
      "a\t³³\tm\t-\tɐ\t³³\n",
      "1.5000000000000002\n",
      "\n",
      "Guangzhou Lahu\n",
      "-\t-\t-\tf\tu\ti\t⁵³\n",
      "m\tə\t³³\t-\t-\te\t³³\n",
      "0.5999999999999999\n",
      "\n",
      "Guangzhou Meixian\n",
      "f\tu\ti\t⁵³\n",
      "f\to\ti\t³³\n",
      "26.0\n",
      "\n",
      "Junuo Lahu\n",
      "-\ta\t³³\tm\tɐ\t³³\n",
      "m\tə\t³³\t-\te\t³³\n",
      "14.799999999999999\n",
      "\n",
      "Junuo Meixian\n",
      "a\t³³\tm\t-\tɐ\t³³\n",
      "-\t-\tf\to\ti\t³³\n",
      "2.8000000000000003\n",
      "\n",
      "Lahu Meixian\n",
      "m\tə\t³³\t-\t-\te\t³³\n",
      "-\t-\t-\tf\to\ti\t³³\n",
      "1.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pairwise alignment, SCA model\n",
    "pairs=combinations(wl.doculect,2)\n",
    "for c in pairs:\n",
    "    seqA=wl.get_list(doculect=c[0],entry='ipa')[0]\n",
    "    seqB=wl.get_list(doculect=c[1],entry='ipa')[0]\n",
    "    p=Pairwise(seqA,seqB, merge_vowels=False)\n",
    "    p.align(model='sca')\n",
    "    print(c[0],c[1])\n",
    "    print('{0}\\n'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressive method\n",
      "x\tuei\t⁵⁵\t-\t-\t-\n",
      "f\tui\t⁵³\t-\t-\t-\n",
      "-\ta\t³³\tm\tɐ\t³³\n",
      "m\tə\t³³\t-\te\t³³\n",
      "f\toi\t³³\t-\t-\t-\n",
      "library method\n",
      "-\t-\tx\tuei\t⁵⁵\t-\t-\n",
      "-\t-\tf\tui\t⁵³\t-\t-\n",
      "a\t³³\tm\tɐ\t³³\t-\t-\n",
      "-\t-\tm\tə\t³³\te\t³³\n",
      "-\t-\tf\toi\t³³\t-\t-\n",
      "Merge_vowels=False and progressive method\n",
      "-\t-\t-\tx\tu\te\ti\t⁵⁵\n",
      "-\t-\t-\tf\t-\tu\ti\t⁵³\n",
      "-\ta\t³³\tm\t-\t-\tɐ\t³³\n",
      "m\tə\t³³\t-\t-\t-\te\t³³\n",
      "-\t-\t-\tf\t-\to\ti\t³³\n",
      "Merge_vowerls=False and library method\n",
      "-\t-\t-\t-\tx\tu\te\ti\t⁵⁵\n",
      "-\t-\t-\t-\tf\t-\tu\ti\t⁵³\n",
      "-\ta\t³³\tm\t-\t-\t-\tɐ\t³³\n",
      "m\tə\t³³\t-\t-\t-\t-\te\t³³\n",
      "-\t-\t-\t-\tf\t-\to\ti\t³³\n",
      "Global alignment analysis with Needleman-Wunsh algnment mode\n",
      "-\t-\t-\tx\tu\te\ti\t⁵⁵\n",
      "-\t-\t-\tf\t-\tu\ti\t⁵³\n",
      "-\ta\t³³\tm\t-\t-\tɐ\t³³\n",
      "m\tə\t³³\t-\t-\t-\te\t³³\n",
      "-\t-\t-\tf\t-\to\ti\t³³\n",
      "Global alignment analysis which seeks to maximize local similarities\n",
      "x\tu\te\ti\t⁵⁵\t-\t-\t-\t-\t-\n",
      "f\tu\ti\t-\t⁵³\t-\t-\t-\t-\t-\n",
      "-\ta\t-\t-\t³³\tm\tɐ\t³³\t-\t-\n",
      "-\t-\t-\t-\t-\tm\tə\t³³\te\t³³\n",
      "f\to\ti\t-\t³³\t-\t-\t-\t-\t-\n"
     ]
    }
   ],
   "source": [
    "# Multiple alignment part\n",
    "from lingpy import *\n",
    "wl=Wordlist('../data/S07_toy-data.tsv')\n",
    "# progressive alignment-simple\n",
    "msa_simple=Multiple(wl.ipa[0])\n",
    "msa_unmerge=Multiple(wl.ipa[0], merge_vowels=False)\n",
    "print('Progressive method')\n",
    "print(msa_simple.align('progressive'))\n",
    "print('library method')\n",
    "print(msa_simple.align('library'))\n",
    "print('Merge_vowels=False and progressive method')\n",
    "print(msa_unmerge.align('progressive'))\n",
    "print('Merge_vowerls=False and library method')\n",
    "print(msa_unmerge.align('library'))\n",
    "print('Global alignment analysis with Needleman-Wunsh algnment mode')\n",
    "print(msa_unmerge.align('progressive',mode='global'))\n",
    "print('Global alignment analysis which seeks to maximize local similarities')\n",
    "print(msa_unmerge.align('progressive',mode='dialign'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great news! LingPy can also use tokenized data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untokenize data looks like this:\n",
      "xuei⁵⁵\n",
      "fui⁵³\n",
      "a³³mɐ³³\n",
      "mə³³e³³\n",
      "foi³³\n",
      "Tokenized data looks like this:\n",
      "['x', 'u', 'e', 'i', '⁵⁵']\n",
      "['f', 'u', 'i', '⁵³']\n",
      "['a', '³³', 'm', 'ɐ', '³³']\n",
      "['m', 'ə', '³³', 'e', '³³']\n",
      "['f', 'o', 'i', '³³']\n",
      "multiple alignment for the tokenize data\n",
      "-\t-\t-\tx\tu\te\ti\t⁵⁵\n",
      "-\t-\t-\tf\t-\tu\ti\t⁵³\n",
      "-\ta\t³³\tm\t-\t-\tɐ\t³³\n",
      "m\tə\t³³\t-\t-\t-\te\t³³\n",
      "-\t-\t-\tf\t-\to\ti\t³³\n"
     ]
    }
   ],
   "source": [
    "print('untokenize data looks like this:')\n",
    "for w in wl.ipa[0]:\n",
    "    print(w)\n",
    "print('Tokenized data looks like this:')\n",
    "for t in wl.tokens[0]:\n",
    "    print(t)\n",
    "print('multiple alignment for the tokenize data')\n",
    "msa_tokens=Multiple(wl.tokens[0])\n",
    "print(msa_tokens.align('progressive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Further Information on LingPy (LingPy without Pain)\n",
    "\n",
    "In case you run into difficulties, there are some resource to guide you. \n",
    "\n",
    "* a new tutorial on LingPy was recently published: https://github.com/lingpy/lingpy-tutorial \n",
    "* we will announce new information on LingPy in our new blog at https://calc.hypotheses.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analysing and Correcting Alignments with EDICTOR\n",
    "\n",
    "the Etymological DICtionary ediTOR (EDICTOR), a free, interactive, web-based tool designed to aid\n",
    "historical linguists in creating, editing,\n",
    "analysing, and publishing etymological\n",
    "datasets. The EDICTOR offers interactive solutions for important tasks in historical linguistics, including facilitated input\n",
    "and segmentation of phonetic transcriptions, quantitative and qualitative analyses\n",
    "of phonetic and morphological data, enhanced interfaces for cognate class assignment and multiple word alignment, and\n",
    "automated evaluation of regular sound correspondences. As a web-based tool written in JavaScript, the EDICTOR can be\n",
    "used in standard web browsers across all\n",
    "major platforms.\n",
    "\n",
    "#### 3.2.1 Preparing Data\n",
    "\n",
    "Our input file for EDICTOR is the file we also used for the alignments. If you want to prepare data to be used from within EDICTOR. What is important is that the first row of the test file lists the ID of the words, and that there are at least two further columns, one with a CONCEPT and one with a DOCULECT. EDICTOR theoretically accepts other column names, but don't mess with them, as the behavior may be unexpected. What is also needed is a column with TOKENS (segmentized IPA entries) that list the translation for each item. For alignments, we further need a COGID column (we will discuss cognate coding in detail in Session 8), and a (potentially) empty column for the ALIGNMENT. If ALIGNMENT is not yet there in the data, EDICTOR will automatically create it, if you align strings manually. But we recommend to always start from a file that has all those columns already. EDICTOR will add values in the cell, but should not manipulate the columns.\n",
    "\n",
    "#### 3.2.2 Loading Data into EDICTOR\n",
    "\n",
    "Loading data into EDICTOR is straightforward, either per drag-and-drop, or by pressing the select button. The file should be a plain text file, no Excel or similar. In order to create such a file from Excel, just open an empty text file on your computer, mark all cells in your Excel spreadsheet (make sure you added all column names and don't have empty rows in your data), copy them, and paste them in the text file. \n",
    "\n",
    "The following video shows, how you can load the data by pressing the select button:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s7-edictor-1.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Aligning Data with EDICTOR\n",
    "\n",
    "Aligning data is straightforward. By clicking on cells of the column COGID with the right mouse button, you can select a given cognate set to be aligned. A pop-up opens showing the (usually initially unaligned) alignment. By clicking either on EDIT or on ALIGN, the alignment can be actively edited by the user. EDIT won't change the initial alignment, but ALIGN will use a very simple base-line algorithm for multiple alignment to pre-align the data. Once you entered the EDIT mode, you can push and pull the sounds and arrange them in a very simple manner: pressing on a sound will insert a gap symbol before that sound, so push it to the right. Pressing on a gap will delete that gap. You can further mark parts that are considered as \"unalignable\" and could be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s7-edictor-2.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2.4 Saving data with EDICTOR\n",
    "\n",
    "Saving is a bit less intuitive at the moment. You actually need to \"download\" your data. You need to first press the SAVE button on the top right corner (shortcut on non-mac systems: CONTROL + S), and then press the EXPORT button next to it, to do what looks like a download of the data, but is in fact only a way to transfer data in a save way from the sandbox of the webbrowser to your computer. EDICTOR will add some more text at the end of your original file, to make sure it remembers your previous settings. These lines can be safely deleted, if you prefer. EDICTOR will re-open the file without problems, but not have remembered, for example, what concepts you were working on, or different specific settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s7-edictor-3.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
