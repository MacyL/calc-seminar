{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Linguistic Data Formats and Beyond (Christoph Rzymski and Nathanael E. Schweikhard and Tiago Tresoldi and Johann-Mattis List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Welcome to this session, in which we will cover Orthographic Profiles and the Cross-Linguistic Data Formats (CLDF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Orthographic Profiles\n",
    "\n",
    "## 2.1 The Idea\n",
    "\n",
    "Say you have a huge amount of linguistic data that all uses the same transcription system, and you want to convert that data into IPA.\n",
    "\n",
    "Orthographic Profiles provide an easy method of replacing letters and letter combinations with the corresponding IPA-symbols by using a simple replacement table. They work like Finite State Automata: They look at each letter of the data from left to right, check whether it is in the replacement table, and replace it with the corresponding IPA. They also take letter combinations into account, and therefore they go through the table from the longest to the shortest entry.\n",
    "\n",
    "For example if you would want to convert a few Castilian Spanish words into a fairly phonemic IPA rendering:\n",
    "\n",
    "*hola llamar amiga*\n",
    "\n",
    "you would need the following correspondences:\n",
    "\n",
    "| Grapheme | IPA  |\n",
    "|----------|------|\n",
    "| h        | NULL |\n",
    "| o        | o    |\n",
    "| l        | l    |\n",
    "| a        | a    |\n",
    "| ll       | ʎ    |\n",
    "| m        | m    |\n",
    "| r        | ɾ    |\n",
    "| g        | ɣ    |\n",
    "| i        | i    |\n",
    "\n",
    "to gain the output:\n",
    "o l a # ʎ a m a ɾ # a m i ɣ a\n",
    "\n",
    "The &lt;ll> in llamar will get converted correctly into /ʎ/ instead of /l/ since the &lt;ll>, being longer than &lt;l>, gets checked first, so you don't need to worry about putting the rows into any specific order.\n",
    "\n",
    "For deleting a letter since it’s not pronounced (like &lt;h>) you should use the value NULL in the IPA column.\n",
    "\n",
    "## 2.2 Using Your Orthographic Profile in Python\n",
    "\n",
    "For using the profile you created, you first need to turn the table into a simple tab-separated-values-file, e.g. by copy-pasting it from the spreadsheet-software into an editor like Notepad++ and then saving it with the file-ending .tsv.\n",
    "\n",
    "Afterwards you can use the following Python-code to turn your data into the correct IPA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o l a # ʎ a m a ɾ # a m i ɣ a\n"
     ]
    }
   ],
   "source": [
    "# import relevant modules\n",
    "from segments.tokenizer import Tokenizer\n",
    "\n",
    "# load the tokenizer object\n",
    "tk = Tokenizer('../data/orthographic_profile1.tsv')\n",
    "\n",
    "# convert a string to test it\n",
    "print(tk('hola llamar amiga', column='IPA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which should give you the output mentioned above.\n",
    "\n",
    "If the pronunciation depends on whether the letter is at the beginning or end of the word or somewhere in the middle, you can use the symbols ^ and $ for that.\n",
    "\n",
    "So if we want to also include a word that starts with &lt;g> (which is pronounced differently from a &lt;g> in the middle of a word in Spanish) we could add the following row. We also need to add additional rows for the word border signs:\n",
    "\n",
    "| Grapheme | IPA  |\n",
    "|----------|------|\n",
    "| ^g       | g    |\n",
    "| ^        | NULL |\n",
    "| $        | NULL |\n",
    "\n",
    "We then also need to prepare the input data to include word borders, but Python can do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant modules\n",
    "from segments.tokenizer import Tokenizer\n",
    "import csv\n",
    "\n",
    "# load the tokenizer object\n",
    "tk = Tokenizer('../data/orthographic_profile2.tsv')\n",
    "\n",
    "# read input file\n",
    "with open('../data/input_file.tsv') as reader:\n",
    "    reader = csv.DictReader(reader, delimiter=\"\\t\")\n",
    "    words = [row['FORM'] for row in reader] \n",
    "    \n",
    "# add the word borders\n",
    "    words = ['^' + word + '$' for word in words]\n",
    "\n",
    "# apply the orthographic profile to the file\n",
    "# and save the output in a file\n",
    "    with open('../data/output_file.tsv', 'w') as handler:\n",
    "        for word in words:\n",
    "            handler.write(tk(word,column='IPA'))\n",
    "            handler.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you also see how you input and output a file instead of having the input and output only in the terminal.\n",
    "\n",
    "If your input data is already really close to IPA and only needs a bit of cleaning up, you can also automatically create an Orthographic Profile with the help of Lingpy in the terminal by just giving it your data in one column named \"IPA\" in a tsv-file:\n",
    "\n",
    "```\n",
    "lingpy profile -i data/input_file.tsv -o data/simple_profile.tsv --column=ipa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file is then an Orthographic Profile which you can use the same as one you have created manually. It might contain a few mistakes, though, so you should check whether everything was recognized correctly.\n",
    "\n",
    "Further information on how to create Orthographic Profiles automatically can be found here: http://lingpy.org/docu/sequence/profile.html\n",
    "\n",
    "## 2.3 More Complex Profiles\n",
    "\n",
    "So far this sounds pretty easy. There however are some caveats.\n",
    "\n",
    "Orthographic Profiles need to be prepared individually not only for each language but also for each transcription system.\n",
    "\n",
    "They work well if there is a 1 to 1 correspondence between grapheme and sound. But they are a bit tedious to create if the pronunciation depends on the surrounding letters as that can easily turn into a long list of letter combinations you need to include.\n",
    "\n",
    "One easy example for this was the &lt;ll> above, here is another one: In Spanish, the letter c gets pronounced differently depending on the letter afterwards. Normally it's /k/, but before high vowels it's /θ/ (depending on the dialect). Therefore you need to add the following rows to account for this:\n",
    "\n",
    "| Grapheme | IPA  |\n",
    "|----------|------|\n",
    "| c        | k    |\n",
    "| ci       | θi    |\n",
    "| ce       | θe    |\n",
    "\n",
    "Spanish orthography being as regular as it is, making a full Orthographic Profile for it wouldn't be too difficult a task (at least for a phonemic rendering of standard dialects). But in other orthographic systems, if there are irregular exceptions to the pronunciation you then need to list each of those individually. For example an Orthographic Profile for English would be nearly impossible.\n",
    "\n",
    "Furthermore, Orthographic Profiles are not capable of handling things that are not visible in the orthography (which often includes accent, morpheme borders, tone,...), and therefore also not pronunciation differences that depend on this information. If you have data depending on things like that you first need to add this kind of information to the input (often manually).\n",
    "\n",
    "E.g. if you have some German input in standard orthography you would need to first mark the morpheme borders (e.g. with .) and, especially, also mark the vowel length in some fashion.\n",
    "\n",
    "Otherwise your computer wouldn't be able to tell that there is a glottal stop before the second &lt;a> in *Hausarzt* but not in *Bausatz*. Nor would it be able to figure out that the &lt;a> in *Tat* is long but the &lt;a> in *hat* is short.\n",
    "\n",
    "Therefore, the usefulness of the method of Orthographic Profiles hugely depends on what kind of data you are dealing with, and also how exact the IPA needs to be, but can be very helpful in a lot of contexts. It basically depends on whether it's more effort to create the Orthographic Profile or to transliterate all the linguistic data manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Cross-Linguistic Data\n",
    "\n",
    "## 3.1 The Current Situation\n",
    "\n",
    "In order to annotate linguistic data, scholars often make use of ad-hoc formats based on spreadsheet software (Excel, LibreOffice, GoogleSheets). The most common format, of which many linguists also claim that it is the most universal and most easy-to-understand one, is the one according to which rows represent concepts and columns represent languages. This format essentially reserves one column for one language in the spreadsheet, and one row for one concept. The language names are given in the first row, and the concept labels are given in the first column of the spreadsheet.\n",
    "\n",
    "<style>img[alt=\"Table 1: Tabular data format with languages in columns and concepts in rows.\"]{width:400px;}</style>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;hand&quot;</th>\n",
    "    <td>hænd</td>\n",
    "    <td>hant</td>\n",
    "    <td>hɑnt</td>\n",
    "    <td>hʌnˀ</td>\n",
    "    <td>hanːd</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;ashes&quot;</th>\n",
    "    <td>æʃ</td>\n",
    "    <td>aʃə</td>\n",
    "    <td>ɑs</td>\n",
    "    <td>asg</td>\n",
    "    <td>asːka</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>bɑːrk</td>\n",
    "    <td>rɪndə</td>\n",
    "    <td>bɑst</td>\n",
    "    <td>bɑːg</td>\n",
    "    <td>barːk</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>...</th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "This format does not only lack flexibility, as there is only one piece of information that we can give for each concept in a given language, it is also getting more and more impractical if we are dealing with many different languages, as it will be extremely hard to inspect them on a screen (scrolling horizontally is always harder for inspection than scrolling vertically).\n",
    "\n",
    "Despite the shortcomings, this format, or any variant of it, is one of the most widespread forms in which language data is annotated nowadays. The problem of adding essential information on cognacy, or allowing for synonyms is again mostly handled in an ad-hoc manner. Some scholars add additional rows for the same concept in order to allow to add more than one word per meaning and per language:\n",
    "\n",
    "<!-- ![Table 2: Tabular data format with additional rows for synonym rendering.](img/table-2.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 2: Tabular data format with additional rows for synonym rendering.\"]{width:500px;}</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>bɑːrk</td>\n",
    "    <td>rɪndə</td>\n",
    "    <td>bɑst</td>\n",
    "    <td>bɑːg</td>\n",
    "    <td>barːk</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td></td>\n",
    "    <td>bɔrkə</td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Some scholars use commata or other separators to add the same entry in the same cell:\n",
    "\n",
    "<!-- ![Table 3: Multiple synonyms in the same cell.](images/table-3.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 3: Multiple synonyms in the same cell.\"]{width:450px;}</style>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>bɑːrk</td>\n",
    "    <td>rɪndə, bɔrkə</td>\n",
    "    <td>bɑst</td>\n",
    "    <td>bɑːg</td>\n",
    "    <td>barːk</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "And some scholars add another column for the language which shows the synonym:\n",
    "\n",
    "<!-- ![Table 4: Additional column for language to render synonyms.](images/table-4.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 4: Additional column for language to render synonyms.\"]{width:450px;}</style>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"6\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>German (b)</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>bɑːrk</td>\n",
    "    <td>rɪndə</td>\n",
    "    <td>bɔrkə</td>\n",
    "    <td>bɑst</td>\n",
    "    <td>bɑːg</td>\n",
    "    <td>barːk</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "For people concerned with a consistent representation of knowledge, this is a nightmare, but the nightmare gets even more frightening, when it comes to the annotation of cognate sets. Here, people have been proving an incredible amount of phantasy in creating solutions that are computationally not only difficult to track, but also extremely prone to errors. Scholars have been using colors:\n",
    "\n",
    "<!-- ![Table 5: Color-based annotation of cognate sets.](images/table-5.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 5: Color-based annotation of cognate sets.\"]{width:450px;}</style>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td style=\"background-color:LightYellow;\">bɑːrk</td>\n",
    "    <td style=\"background-color:LightBlue;\">rɪndə</td>\n",
    "    <td style=\"background-color:LightGreen;\">bɑst</td>\n",
    "    <td style=\"background-color:LightYellow;\">bɑːg</td>\n",
    "    <td style=\"background-color:LightYellow;\">barːk</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <td style=\"background-color:LightYellow;\">bɔrkə</td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "They often even just put the information on cognacy in a separate sheet, which makes it incredibly difficult to compare their judgments, especially when then number of language exceeds a handful:\n",
    "\n",
    "<!-- ![Table 6: Multi-sheet-based annotation of cognate sets.](images/table-6.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 6: Multi-sheet-based annotation of cognate sets.\"]{width:800px;}</style>\n",
    "<table>\n",
    "  <tr><th>Sheet 1</th><th>Sheet 2</th></tr>\n",
    "  <tr><td>\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>bɑːrk</td>\n",
    "    <td>rɪndə, bɔrkə</td>\n",
    "    <td>bɑst</td>\n",
    "    <td>bɑːg</td>\n",
    "    <td>barːk</td>\n",
    "  </tr>\n",
    "  </table></td>\n",
    "  <td>\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>A</td>\n",
    "    <td>B, A</td>\n",
    "    <td>C</td>\n",
    "    <td>A</td>\n",
    "    <td>A</td>\n",
    "  </tr>\n",
    "  </table>\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "At times, they may even binarise the data manually, which is even more dangerous, as it is almost guaranteed that manually binarising cognate sets will yield errors (not to speak of the waste of time and the fact that one cannot trace the characters back when carrying out phylogenetic analyses).\n",
    "\n",
    "<!-- ![Table 7: Binary annotation of cognate sets in multiple sheets.](images/table-7.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 7: Binary annotation of cognate sets in multiple sheets.\"]{width:500px;}</style>\n",
    "<table>\n",
    "  <tr><th>Sheet 1</th><th>Sheet 2</th></tr>\n",
    "  <tr><td>\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Concepts</th>\n",
    "    <th colspan=\"5\">Languages</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <th>German</th>\n",
    "    <th>Dutch</th>\n",
    "    <th>Danish</th>\n",
    "    <th>Swedish</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>&quot;bark&quot;</th>\n",
    "    <td>bɑːrk</td>\n",
    "    <td>rɪndə, bɔrkə</td>\n",
    "    <td>bɑst</td>\n",
    "    <td>bɑːg</td>\n",
    "    <td>barːk</td>\n",
    "  </tr>\n",
    "  </table></td>\n",
    "  <td>\n",
    "  <table>\n",
    "  <tr><th>Characters</th><th>A</th><th>B</th><th>C</th></tr>\n",
    "  <tr>\n",
    "    <th>English</th>\n",
    "    <td>1</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th>German</th><td>1</td><td>1</td><td>0</td></tr>\n",
    "  <tr><th>Dutch</th><td>0</td><td>0</td><td>1</td></tr>\n",
    "  <tr><th>Danish</th><td>1</td><td>0</td><td>0</td></tr>\n",
    "  <tr><th>Swedish</th><td>1</td><td>0</td><td>0</td></tr>\n",
    "  </table></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 More consistent ways of data representation\n",
    "\n",
    "\n",
    "Software packages like STARLING try to circumvent the above-mentioned problems by allowing for additional columns which add additional information for the same language. LingPy and EDICTOR, however, employ a different approach which greatly increases the flexibility of the format. The major principle of this approach is to reserve one row in the spreadsheet for exactly one word form. Additional information for each word form is provided in additional columns (which can be flexibly added by the user both in LingPy and EDICTOR). The content of each column in a LingPy/EDICTOR-spreadsheet is given in the header of the file, with the first column being reserved for a numeric ID which should be greater than 0:\n",
    "\n",
    "<!-- ![Table 9: Polynesian data example for standard format in LingPy and EDICTOR.](images/table-8.png) -->\n",
    "\n",
    "<style>img[alt=\"Table 9: Polynesian data example for standard format in LingPy and EDICTOR.\"]{width:700px;}</style>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>ID</th>\n",
    "    <th>DOCULECT</th>\n",
    "    <th>CONCEPT</th>    \n",
    "    <th>VALUE</th>\n",
    "    <th>FORM</th>\n",
    "    <th>TOKENS</th>\n",
    "    <th>BORROWING</th>\n",
    "    <th>COGID</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3631</td>\n",
    "    <td>East_Futuna</td>\n",
    "    <td>above</td>\n",
    "    <td>à/luga/</td>\n",
    "    <td>luga</td>\n",
    "    <td>l u g a</td>\n",
    "    <td>0</td>\n",
    "    <td>1382</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>284</td>\n",
    "    <td>Wallisian</td>\n",
    "    <td>above</td>\n",
    "    <td>'o/luga/</td>\n",
    "    <td>luga</td>\n",
    "    <td>l u g a</td>\n",
    "    <td>1</td>\n",
    "    <td>1382</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5391</td>\n",
    "    <td>Futuna_Aniwa</td>\n",
    "    <td>above</td>\n",
    "    <td>weihlunga</td>\n",
    "    <td>weihlunga</td>\n",
    "    <td>w e i + ʰl u ŋ a</td>\n",
    "    <td>0</td>\n",
    "    <td>1382</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>761</td>\n",
    "    <td>Maori</td>\n",
    "    <td>above</td>\n",
    "    <td>i runga</td>\n",
    "    <td>i runga</td>\n",
    "    <td>i _ r u ŋ a</td>\n",
    "    <td>0</td>\n",
    "    <td>1382</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3332</td>\n",
    "    <td>North_Marquesan</td>\n",
    "    <td>above</td>\n",
    "    <td>'una</td>\n",
    "    <td>'una</td>\n",
    "    <td>ʔ u n a</td>\n",
    "    <td>0</td>\n",
    "    <td>1382</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4214</td>\n",
    "    <td>Mele-Fila</td>\n",
    "    <td>all</td>\n",
    "    <td>euči</td>\n",
    "    <td>euči</td>\n",
    "    <td>e u tʃ i</td>\n",
    "    <td>0</td>\n",
    "    <td>1115</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3917</td>\n",
    "    <td>Pukapuka</td>\n",
    "    <td>all</td>\n",
    "    <td>katoa(toa)</td>\n",
    "    <td>katoa</td>\n",
    "    <td>k a + t o a</td>\n",
    "    <td>0</td>\n",
    "    <td>293</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>560</td>\n",
    "    <td>Proto-Polynesian</td>\n",
    "    <td>yellow</td>\n",
    "    <td>*reŋareŋa, *felo(-felo)</td>\n",
    "    <td>*reŋareŋa</td>\n",
    "    <td>r e ŋ a + r e ŋ a</td>\n",
    "    <td>0</td>\n",
    "    <td>162</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>561</td>\n",
    "    <td>Proto-Polynesian</td>\n",
    "    <td>yellow</td>\n",
    "    <td>*reŋareŋa, *felo(-felo)</td>\n",
    "    <td>*felo</td>\n",
    "    <td>f e l o</td>\n",
    "    <td>0</td>\n",
    "    <td>230</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "While this format seems to be rather redundant on first sight, it offers a so much greater degree of flexibility that all linguists who started to seriously test this kind of data representation quickly understand the advantages. What you need to keep in mind is that the number of columns is theoretically unlimited. So you can easily add your own columns which you want to use for either enhanced ways to annotated and model your data, or to add notes in prose which you can later include in your publication. You can add sources, and you can be very detailed, listing the page number for each word form to trace from which source it was originally taken. The possibilities are virtually unlimited, once you get a clearer understanding of this way to handle linguistic data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Simple but important rules for data consistency\n",
    "\n",
    "In the previous sections we have tried to show that the ad-hoc formats employed by linguists for data collection usually have huge disadvantages in terms of transparancy and inter-operability, and we recommend all users to take the time to read more about the formats underlying LingPy, EDICTOR, but also the *Cross-Linguistic Data Formats* initiative ([Forkel et al. 2017](http://bibliography.lingpy.org?key=Forkel2017a), see http://cldf.clld.org). These formats are to a large degree compatible, for LingPy and EDICTOR, they are almost identical, and will be introduced below. \n",
    "\n",
    "The most obvious failure in data annotation that many scholars commit when preparing their data in Excel or other spreadsheet software is that they include multiple different types of information into one cell. Thus, if a word has a variant, scholars will place it into one cell in their spreadsheet software and separate the entries by a comma, a colon, a tilde, a dash, or at times even by a back-slash, often even using all of these separators inconsistently for the same dataset. A first and general rule that people creating data must understand and follow, is that \n",
    "\n",
    "  **`1.` Only one type of information should be put into one cell in a spreadsheet.** \n",
    "\n",
    "This rule is non-negotiable, as in our experience with a huge number of differently coded datasets, scholars necessarily make annotation errors, even if they try to be consistent. Computers are not like humans, and if you want to profit from computers to ease your work, assume that they cannot interpret whether you use a comma and a colon without semantic difference when listing word variants or whether you do it on purpose. In fact, humans are also unlikely to understand this, unless it was them who created the data. \n",
    "\n",
    "A more general rule deriving from this first rule is the rule that \n",
    "\n",
    "  **`2.` All information valid for a given analysis needs to be consistently annotated.**\n",
    "\n",
    "This means, for example, that, if root alternation is important for your reconstruction and cognate decisions, you need to think how to model this in consistent markup. If your data contains reflexes of an alternating protoform *&ast;ka- vs. &ast;ku-*, for example, it is not sufficient to simply write *&ast;ka- vs. &ast;ku-* and listing the reflexes, assuming that your readers will understand which reflex stems from which of the two alternants. Instead, Two proto-forms should be listed, the variants should be assigned to the correct proto-form from which they evolve, and the additional information should be given that *&ast;ka-* and *&ast;ku-* are variants of the same root. This practice is rarely followed systematically in etymological dictionaries, and therefore also often disregarded in databases, but it is clear that it is the only way to transparently list what reflex stems from which proto-variant. In fact, this is not a matter of a more computational approach to historical linguistics, but rather a matter of improving on our common practice in historical linguistics, which has for too long a time been based on lax guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 The CLDF Initiative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Introduction to CLDF\n",
    "\n",
    "The CLDF initiative is an attempt to standardize cross-lingustic data. This attempt can be traced back almost twenty years (Dimitriadis 2001). With the efforts to standardize the software stack for cross-linguistic databases within the CLLD project, we became aware of the need for a general data model that could be used to represent cross-linguistic data in form of word lists and typological surveys. This data model aims to provide a convenient form that should ease the comparison across datasets and applications.\n",
    "\n",
    "All of this culminated in a series of workshops, hosted by the Max Planck Institute for Psycholinguistics in Nijmegen\n",
    "(Language Comparison with Linguistic Databases, 2014), the Max Planck Institute for Evolutionary Anthropology in Leipzig (Language Comparison with Linguistic Databases 2, 2015), the Lorentz Center in Leiden\n",
    "(Capturing Phylogenetic Algorithms for Linguistics, 2015), and numerous follow-up workshops organized\n",
    "as part of the Glottobank project (http://glottobank.org), an international research consortium\n",
    "established to document and understand the world's linguistic diversity, funded by the Max Planck Institute\n",
    "for the Science of Human History in Jena (2014-2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 CLDF Idea\n",
    "\n",
    "Generally speaking, data in historical and typological linguistic research have a (seemingly) simple structure:\n",
    "\n",
    "* `languages` have\n",
    "* `features` that, in turn, have different\n",
    "* `values`.\n",
    "\n",
    "The triple (or set of triples in a complete study) of `(language, feature, value)` appears easy and straightforward enough, but carries multiple potential fallacies. Can you think of any or have you experienced any in your own reaserch experiences?\n",
    "\n",
    "The data model underlying the [CLDF specification](https://github.com/cldf/cldf) aims to be a remedy to these (potential) issues. In general, CLDF aims to make linguistic data `FAIR`:\n",
    "\n",
    "* findable,\n",
    "* accessible,\n",
    "* interoperable,\n",
    "* and re-usable.\n",
    "\n",
    "CLDF places a heavy emphasis on the re-usability aspect, since we probably all know stories of interesting linguistic data sets that have been lost to time, coding issues, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 CLDF Data Model\n",
    "\n",
    "How does the CLDF specification aim to achieve more FAIRness for linguistic data?\n",
    "\n",
    "First, the CLDF specification doesn't want and need unnecessary complexity and sticks to the previously established triple of `(language, feature, value)` but refines the notion of the individual entities:\n",
    "\n",
    "* `languages` are `languoids` and represent the object under investigation\n",
    "* `features` are `parameters` and entail the comparative concepts that can be compared across different `languoids`\n",
    "* `values` are `values` and represent measurements for `languoid`-`parameter` pairs\n",
    "\n",
    "One additional layer expands this model as each triple can be and should be linked to one or multiple sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 CLDF Ontology\n",
    "\n",
    "For our purposes, the CLDF Ontology serves the function of having a catalogue of entities that can exist within a CLDF data set. More generally speaking, the ontology lists things that can exists within a CLDF data set and specifies their relationship between each other.\n",
    "\n",
    "Visiting [the CLDF ontology web site](http://cldf.clld.org/v1.0/terms.rdf) with a reasonably modern browser (Firefox recommended!) nicely 'illustrates' the CLDF ontology and its components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 CLDF in the Wild\n",
    "\n",
    "To support workig with real-world data, CLDF is separated into `modules` and `components`, which in turn allow tackling and modelling real-world problems.\n",
    "\n",
    "The top-most order of organization of a CLDF data set is the `module` that is being used. CLDF provides support for multiple different `modules`:\n",
    "\n",
    "* [`Wordlist`](https://github.com/cldf/cldf/tree/master/modules/Wordlist)\n",
    "* [`StructureDataset`](https://github.com/cldf/cldf/tree/master/modules/StructureDataset)\n",
    "* [`Dictionary`](https://github.com/cldf/cldf/tree/master/modules/Dictionary)\n",
    "* [`ParallelText`](https://github.com/cldf/cldf/tree/master/modules/ParallelText)\n",
    "* [`Generic`](https://github.com/cldf/cldf/tree/master/modules/Generic)\n",
    "\n",
    "A finer-grained control over a module's content can be achieved by employing the different `components` the CLDF specification has to offer:\n",
    "\n",
    "* [`Language Metadata`](https://github.com/cldf/cldf/tree/master/components/languages)\n",
    "* [`Parameter Metadata`](https://github.com/cldf/cldf/tree/master/components/parameters)\n",
    "* [`Values`](https://github.com/cldf/cldf/tree/master/components/values)\n",
    "* [`Codes`](https://github.com/cldf/cldf/tree/master/components/codes)\n",
    "* [`Entries`](https://github.com/cldf/cldf/tree/master/components/entries)\n",
    "* [`Senses`](https://github.com/cldf/cldf/tree/master/components/senses)\n",
    "* [`Examples`](https://github.com/cldf/cldf/tree/master/components/examples)\n",
    "* [`Forms`](https://github.com/cldf/cldf/tree/master/components/forms)\n",
    "* [`Cognates`](https://github.com/cldf/cldf/tree/master/components/cognates)\n",
    "* [`CognateSets`](https://github.com/cldf/cldf/tree/master/components/cognatesets)\n",
    "* [`Borrowings`](https://github.com/cldf/cldf/tree/master/components/borrowings)\n",
    "* [`Functional Equivalents`](https://github.com/cldf/cldf/tree/master/components/functionalequivalents)\n",
    "* [`Functional Equivalents Sets`](https://github.com/cldf/cldf/tree/master/components/functionalequivalentsets)\n",
    "\n",
    "### 4.5.1 pycldf\n",
    "\n",
    "...\n",
    "\n",
    "### 4.5.2 CLDF & R -- It's just CSV!\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 CLDF Resources\n",
    "\n",
    "You can find more information, tutorials, and examples here:\n",
    "\n",
    "* http://cldf.clld.org/\n",
    "* https://github.com/cldf/cldf\n",
    "* https://github.com/cldf/cldf/tree/master/examples\n",
    "* https://github.com/cldf/pycldf\n",
    "* https://github.com/cldf/cookbook\n",
    "* https://github.com/lexibank (see the individual data set repositories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 What's in It for You?\n",
    "\n",
    "Creating CLDF-compliant data sets might seem daunting and not worth the additional overhead, at first. However, you shouldn't be scared by the wealth of modules and components; for very many basic research scenarios a basic file structure that follows the `Generic` CLDF idea is fine and already brings a multitude of benefits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
