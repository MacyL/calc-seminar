{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-linguistic colexifications\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 The Database of Cross-Linguistic Colexifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 How to Compute Colexifications in Python\n",
    "\n",
    "In the following, we will briefly look how colexifications in a given dataset can be handled with LingPy and with Python in general. \n",
    "\n",
    "Let us start by loading the module from LingPy along with a wordlist of Bai dialects and compute the most frequent colexifications in this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year      \tblood     \t9\n",
      "dry       \tliver     \t8\n",
      "heart     \tnew       \t7\n",
      "sun       \twarm      \t5\n",
      "lie       \tsleep     \t4\n",
      "wind      \tsalt      \t4\n",
      "one       \tnot       \t4\n",
      "stand     \ttree      \t4\n"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from lingpy.meaning.colexification import *\n",
    "\n",
    "wl1 = Wordlist('../data/S10-BAI.tsv')\n",
    "G = colexification_network(wl1, output=None, concept='concept', ipa='ipa')\n",
    "for nA, nB, data in sorted(G.edges(data=True), key=lambda x: x[2]['wordWeight'], reverse=True):\n",
    "    if data['wordWeight'] > 3:\n",
    "        print('{0:10}\\t{1:10}\\t{2}'.format(nA, nB, data['wordWeight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see from this example, is that none of these colexifications seems to reflect the types of semantic associations we find in the CLICS database or other accounts on colexification studies. This is obviously due to the fact that our sample is very small. What is also interesting, however, is that a couple of colexifications reflect what we could interpret as \"genetic markers\" for the group of Bai dialects. The colexification of \"year\" and \"blood\", for example, occurs in all dialects in our sample. It is a clear example of homophony due to phonological merger. \n",
    "\n",
    "At least two of those mergers can also be found in a couple of Chinese dialects, especially \"dry\" and \"liver\", \"heart\" and \"new\" (\"year\" and \"blood\" are similar, compare *suì* 岁 and *xiě* 血, but not identical in Mandarin). But this does not prove that Bai dialects are closely related to Chinese, as other scholars would claim that these cases of colexification are due to recent borrowings in the Bai dialects from Chinese (see [Lee and Sagart 2008](:bib:Lee2008)).\n",
    "\n",
    "So far, the usefulness of colexifications that point to homophones rather than polysemies to infer deep or shallow genetic relationships has not yet sufficiently been investigated. Let us just make a small experiment and check what we find in Polynesian languages, given that we have the data already in our folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "husband   \tman/male  \t6\n",
      "wife      \twoman/female\t6\n",
      "Five      \thand      \t6\n",
      "dirty     \tearth/soil\t5\n",
      "One Hundred\tleaf      \t4\n",
      "to see    \tto know, be knowledgeable\t4\n",
      "to kill   \tto hit    \t4\n"
     ]
    }
   ],
   "source": [
    "wl2 = Wordlist('../data/S08_east-polynesian.tsv')\n",
    "G = colexification_network(wl2, output=None, concept='concept', entry='value')\n",
    "for nA, nB, data in sorted(G.edges(data=True), key=lambda x: x[2]['wordWeight'], reverse=True):\n",
    "    if data['wordWeight'] > 3:\n",
    "        print('{0:10}\\t{1:10}\\t{2}'.format(nA, nB, data['wordWeight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we find more instances of polysemies here, although \"One Hundred\" vs. \"Leaf\" are likely to be candidates for homophones. Note that we changed the keyword \"entry\" in this example, since the normal value in unsegmented form, which the algorithm compares for colexifications language-internally, is labelled differently in both datasets (\"ipa\" vs. \"value\").\n",
    "\n",
    "Let us check the Tujia languages in our data-folder to see what we can find there (but we have to lower the threshold, otherwise we don't find any example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "near      \tshort     \t2\n",
      "tooth     \tlouse     \t2\n",
      "eat       \tbite      \t2\n",
      "eat       \tlong      \t2\n",
      "far       \tlong      \t2\n"
     ]
    }
   ],
   "source": [
    "wl3 = Wordlist('../data/S09-data.tsv')\n",
    "G = colexification_network(wl3, output=None, concept='concept', entry='ipa')\n",
    "for nA, nB, data in sorted(G.edges(data=True), key=lambda x: x[2]['wordWeight'], reverse=True):\n",
    "    if data['wordWeight'] > 1:\n",
    "        print('{0:10}\\t{1:10}\\t{2}'.format(nA, nB, data['wordWeight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a small experiment by simply using the colexifications to calculate a distance matrix between the languages. We then calculate the resulting tree retrieved from this distance matrix with the tree retrieved from the comparison of the cognates. For the former we use the `upgma` function of LingPy and a method that computes a distance matrix from colexification data, and for the latter, we just use the `Wordlist.calculate` function that can be applied for all wordlists where there is cognate data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-22 16:06:12,555 [WARNING] Reference tree has already been calculated, force overwrite by setting 'force' to 'True'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    /-Sikaiana\n",
      "          /edge.5--|\n",
      "         |         |          /-Maori\n",
      "         |          \\edge.4--|\n",
      "         |                   |          /-Rapanui\n",
      "         |                    \\edge.3--|\n",
      "         |                             |          /-Hawaiian\n",
      "         |                              \\edge.2--|\n",
      "-root----|                                       |          /-Mangareva\n",
      "         |                                        \\edge.1--|\n",
      "         |                                                 |          /-North_Marquesan\n",
      "         |                                                  \\edge.0--|\n",
      "         |                                                            \\-Tuamotuan\n",
      "         |\n",
      "         |          /-Ra’ivavae\n",
      "          \\edge.7--|\n",
      "                   |          /-Rurutuan\n",
      "                    \\edge.6--|\n",
      "                              \\-Tahitian\n",
      "---\n",
      "                    /-North_Marquesan\n",
      "                   |\n",
      "                   |                    /-Rurutuan\n",
      "          /edge.5--|          /edge.2--|\n",
      "         |         |         |         |          /-Tahitian\n",
      "         |         |         |          \\edge.1--|\n",
      "         |         |         |                   |          /-Hawaiian\n",
      "         |          \\edge.4--|                    \\edge.0--|\n",
      "         |                   |                              \\-Ra’ivavae\n",
      "-root----|                   |\n",
      "         |                   |          /-Maori\n",
      "         |                    \\edge.3--|\n",
      "         |                              \\-Tuamotuan\n",
      "         |\n",
      "         |          /-Rapanui\n",
      "          \\edge.7--|\n",
      "                   |          /-Mangareva\n",
      "                    \\edge.6--|\n",
      "                              \\-Sikaiana\n"
     ]
    }
   ],
   "source": [
    "from lingpy import upgma\n",
    "matrix = compare_colexifications(wl2, entry='value')\n",
    "wl2.calculate('tree', ref='cogid')\n",
    "\n",
    "col_tree = upgma(matrix, taxa=wl2.cols)\n",
    "print(wl2.tree.asciiArt())\n",
    "print('---')\n",
    "print(Tree(col_tree).asciiArt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the differences in the resulting trees, we can easily see that the colexification data does not seem to be very useful for phylogenetic reconstruction.\n",
    "\n",
    "When applying the LingPy function for computing colexifications on large datasets, you should be aware that it may take quite a long time. The reason is that LingPy compares all words against all other words. The current implementation is thus very time-consuming and should be replaced in future versions. A much better way to infer colexifications is to use a Python dictionary as data structure in which the data is consecutively hashed. This will yield a solution that is linear in time (in contrast to exponential, when comparing all words againstt all words). \n",
    "\n",
    "That means, we can retrieve the same information with a much faster function that only iterates once over each word form and adds them to a Python dictionary. If the word form for two concepts is the same, the Python dictionary's values increases, and we can later retrieve all these cases of colexifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year      \tblood     \t9\n",
      "dry       \tliver     \t8\n",
      "heart     \tnew       \t7\n",
      "sun       \twarm      \t5\n",
      "lie       \tsleep     \t4\n",
      "wind      \tsalt      \t4\n",
      "one       \tnot       \t4\n",
      "stand     \ttree      \t4\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "for t in wl1.cols:\n",
    "    colexifications = defaultdict(set)\n",
    "    for idx in wl1.get_list(col=t, flat=True):\n",
    "        colexifications[wl1[idx, 'ipa']].add(wl1[idx, 'concept'])\n",
    "    for key, vals in colexifications.items():\n",
    "        if len(vals) > 1:\n",
    "\n",
    "            for nA, nB in combinations(list(vals), r=2):\n",
    "                try:\n",
    "                    G[nA][nB]['weight'] += 1\n",
    "                except:\n",
    "                    G.add_edge(nA, nB, weight=1)\n",
    "for nA, nB, data in sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True):\n",
    "    if data['weight'] > 3:\n",
    "        print('{0:10}\\t{1:10}\\t{2}'.format(nA, nB, data['weight']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLICS database uses this improved colexification code. Since colexifications are not LingPy's primary concern, it is not clear yet, whether we will remove the current functions from the library in further versions, or otherwise make explicit use of the `pyclics` API. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
