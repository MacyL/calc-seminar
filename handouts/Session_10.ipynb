{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Automatic Detection of Correspondence Patterns (Johann-Mattis List)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Correspondence Patterns in Historical Linguistics\n",
    "\n",
    "### Introduction\n",
    "\n",
    "One of the fundamental insights of early historical linguistic research was that – as\n",
    "a result of systemic changes in the sound system of languages – genetically related\n",
    "languages exhibit structural similarities in those parts of their lexicon which were\n",
    "commonly inherited from their ancestral languages. These similarities surface in form\n",
    "of correspondence relations between sounds from different languages in cognate words.\n",
    "\n",
    "Given the increasing application of automatic methods in historical linguistics after\n",
    "the “quantitative turn” ([Geisler and List 2013](http://bibliography.lingpy.org?key=Geisler2013), 111) in the beginning of this millennium,\n",
    "scholars have repeatedly attempted to either directly infer regular sound correspon-\n",
    "dences across genetically related languages ([Kondrak 2009](http://bibliography.lingpy.org?key=Kondrak2009), [2003](http://bibliography.lingpy.org?key=Kondrak2003); [Brown et al. 2013](http://bibliography.lingpy.org?key=Brown2013)) or integrated the inference into workflows for automatic\n",
    "cognate detection ([List 2014](http://bibliography.lingpy.org?key=List2014d)). What is\n",
    "interesting in this context, however, is that almost all approaches dealing with regular\n",
    "sound correspondences, be it early formal – but classically grounded – accounts (\n",
    "[Grimes and Agard 1959](http://bibliography.lingpy.org?key=Grimes1959); [Hoenigswald 1960](http://bibliography.lingpy.org?key=Hoenigswald1960)) or computer-based methods ([Kondrak 2003](http://bibliography.lingpy.org?key=Kondrak2003); List 2014) only consider sound correspondences between *pairs* of languages.\n",
    "\n",
    "A rare exception can be found in the work of [Anttila](http://bibliography.lingpy.org?key=Anttila1972) (1972, 229-263), who presents\n",
    "the search for regular sound correspondences across multiple languages as the basic tech-\n",
    "nique underlying the comparative method for historical language comparison. Anttila’s\n",
    "description starts from a set of cognate word forms (or morphemes) across the languages\n",
    "under investigation. These words are then arranged in such a way that corresponding\n",
    "sounds in all words are placed into the same column of a matrix. The extraction of\n",
    "regularly recurring sound correspondences in the languages under investigation is then\n",
    "based on the identification of similar patterns recurring across different columns within\n",
    "the cognate sets.\n",
    "\n",
    "The procedure is illustrated in the following figure, where four cognate sets in Sanskrit, Ancient Greek, Latin, and Gothic are shown.\n",
    "\n",
    "![img](img/s10-fig1.png)\n",
    "\n",
    "While it seems trivial to identify sound correspondences across multiple languages\n",
    "from the few examples provided in the figure, the problem can become quite complicated\n",
    "if we add more cognate sets and languages to the comparative sample. Especially the\n",
    "handling of missing reflexes for a given cognate set becomes a problem here, as missing\n",
    "data makes it difficult for linguists to decide which alignment columns to group with each\n",
    "other. This can already be seen from the examples given in Figure 1, where we have two\n",
    "possibilities to group the patterns A, C, E, and F.\n",
    "\n",
    "### Preliminaries on Sound Correspondence Patterns\n",
    "\n",
    "\n",
    "Sound correspondences are most easily defined for pairs of languages. Thus, it is straight-\n",
    "forward to state that German `[`d`]` regularly corresponds to English `[`θ`]`, that German `[`ts`]`\n",
    "regularly corresponds to English `[`t`]`, and that German `[`t`]` corresponds to English `[`d`]`.\n",
    "We can likewise expand this view to multiple languages by adding another Germanic\n",
    "language, such as, for example, Dutch to our comparison, which has `[`d`]` in the case of\n",
    "German `[`d`]` and English `[`θ`]`, `[`t`]` in the case of German `[`ts`]` and English `[`t`]`, and `[`d`]`\n",
    "in the case of German [t] and English [d]. Examples for all forms are given along with\n",
    "proto-forms in Proto-Germanic in the table below.\n",
    "\n",
    "![img](img/s10-tab1.png)\n",
    "\n",
    "The more languages we add to the sample, however, the more complex the picture will\n",
    "get, and while we can state three (basic) patterns for the case of English, German, and\n",
    "Dutch, given in our example, we may get easily more patterns, due to secondary sound\n",
    "changes in the different languages, although we would still reconstruct only three sounds\n",
    "in the proto-language (`[`θ, t, d`]`). Thus, there is a one-to-n relationship between what we\n",
    "interpret as a proto-sound of the proto-language, and the regular correspondence patterns\n",
    "which we may find in our data.\n",
    "\n",
    "While we will reserve the term sound correspondence for\n",
    "pairwise language comparison, we will use the term *sound correspondence pattern* (or\n",
    "simply *correspondence pattern*) for the abstract notion of regular sound correspondences\n",
    "across a set of languages which we can find in the data.\n",
    "\n",
    "### Correspondence Patterns and Proto-Forms\n",
    "\n",
    "Scholars like Meillet ([1908](http://bibliography.lingpy.org?key=Meillet1908), 23) have stated that the core of historical linguistics is not\n",
    "linguistic reconstruction, but the inference of correspondence patterns, emphasizing that\n",
    "'reconstructions are nothing else but the signs by which one points to the correspondences in short form’' However, given the one-to-n relation between proto-sounds and\n",
    "correspondence patterns, it is clear, that this is not quite correct. Having inferred regular\n",
    "correspondence patterns in our data, our reconstructions will add a different level of\n",
    "analysis by further clustering these patterns into groups which we believe to reflect one\n",
    "single sound in the ancestral language.\n",
    "\n",
    "That there are usually more than just one correspondence pattern for a reconstructed\n",
    "proto-sound is nothing new to most practitioners of linguistic reconstruction. Unfortunately, however, linguists do rarely list all possible correspondence patterns exhaustively\n",
    "when presenting their reconstructions, but instead select the most frequent ones, leaving\n",
    "the explanation of weird or unexpected patterns to comments written in prose.\n",
    "\n",
    "### Correspondence Patterns in Classical Linguistic Literature\n",
    "\n",
    "What scholars do instead is providing tables which summarise the correspondence\n",
    "patterns in a rough form, e.g., by showing the reflexes of a given proto-sound in the\n",
    "descendant languages in a table, where multiple reflexes for one and the same language\n",
    "are put in the same cell. An example, taken with modifications from Clackson ([2007](http://bibliography.lingpy.org?key=Clackson2007): 37),\n",
    "is given in the following table.\n",
    "\n",
    "![img](img/s10-tab2.png)\n",
    "\n",
    "### Correspondence Patterns and Alignments\n",
    "\n",
    "In order to infer correspondence patterns, the data must be available in aligned form, that is, we must know which of the\n",
    "sound segments that we compare across cognate sets are assumed to go back to the\n",
    "same ancestral segment. This is illustrated in the following figure, where the cognate sets from the table above are presented in aligned form, following the alignment annotations of LingPy and EDICTOR.\n",
    "\n",
    "![img](img/s10-fig2.png)\n",
    "\n",
    "\n",
    "It is important to keep in mind that strict alignments can only be made of cognate\n",
    "words (or parts of cognate words) that are *directly related*. The notion of directly related\n",
    "word (parts) is close to the notion of orthologs in evolutionary biology ([List 2016](http://bibliography.lingpy.org?key=List2016f)) and\n",
    "refers to words or word parts whose development have not been influenced by secondary\n",
    "changes due to morphological processes.\n",
    "\n",
    "Following evolutionary biology, a given column of an alignment is called an *alignment\n",
    "site* (or simply a site). An alignment site may reflect the same values as we find\n",
    "in a correspondence pattern, and correspondence patterns are usually derived from\n",
    "alignment sites, but in contrast to a correspondence pattern, an alignment site may\n",
    "reflect a correspondence pattern only incompletely, due to missing data in one or more\n",
    "of the languages under investigation.\n",
    "\n",
    "In alignments, \"gaps\" due to missing reflexes of a given cognate set are not the same as\n",
    "the gaps inside an alignment, since the latter are due to the (regular) loss or gain of a\n",
    "sound segment in a given alignment site, while gaps due to missing reflexes may either\n",
    "reflect processes of lexical replacement (List 2014, 37f), or a preliminary stage of research\n",
    "resulting from insufficient data collections or insufficient search for potential reflexes.\n",
    "\n",
    "While we follow the LingPy annotation for gaps in alignments by using the dash as a\n",
    "symbol for gaps in alignment sites, we will use the character Ø (denoting the empty set)\n",
    "to represent missing data in correspondence patterns and alignment sites. This is illustrated in the following figure.\n",
    "\n",
    "![img](img/s10-fig3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 An Algorithm for Automatic Correspondence Pattern Recognition\n",
    "\n",
    "### 2.1 Preliminary Thoughts\n",
    "\n",
    "#### 2.1.1 Compatibility of Alignment Sites\n",
    "\n",
    "If we recall the problem we had in grouping the alignment sites E and F from Figure 1\n",
    "with either A or C, we can see that the general problem of grouping alignment sites to\n",
    "correspondence patterns is their *compatibility*. If we had reflexes for all languages under\n",
    "investigation in all cognate sets, the compatibility would not be a problem, since we\n",
    "could simply group all identical sites with each other, and the task could be considered\n",
    "as solved. However, since it is rather an exception than the norm to have reflexes for all\n",
    "languages under consideration in a number of cognate sets, we will always find alternative\n",
    "possibilities to group our alignment sites in correspondence patterns. In the following, I\n",
    "will assume that two alignment sites are compatible, if they (a) share at least one sound\n",
    "which is not a gap symbol, and (b) do not have any conflicting sounds. We can further\n",
    "weight the compatibility by counting how many sounds are shared among two alignment\n",
    "sites. This is illustrated in the following figure for our four alignment sites A, C, E, and F from\n",
    "the figure above. As we can see from the figure, only two sites are incompatible, namely\n",
    "A and C, as they show different sounds for the reflexes in Gothic. Given that the reflex\n",
    "for Latin is missing in site C, we can further see that C shares only two sounds with E\n",
    "and F.\n",
    "\n",
    "![img](img/s10-fig4.png)\n",
    "\n",
    "#### 2.1.2 Modeling Sound Correspondence Patterns in Networks\n",
    "\n",
    "Having established the concept of alignment site compatibility in the previous section, it\n",
    "is straightforward to go a step further and model alignment sites in form of a network.\n",
    "Here, all sites in the data represent nodes (or vertices), and edges are only drawn between\n",
    "those nodes which are compatible, following the criterion of compatibility outlined in\n",
    "the previous section. We can further weight the edges in the alignment site network, for\n",
    "example, by using the number of matching sounds (where no missing data is encountered)\n",
    "to represent the strength of the connection (but we will disregard weighting in our\n",
    "method). The following figure illustrates how an alignment site network can be created from the compatibility comparison shown in the figure above.\n",
    "\n",
    "![img](img/s10-fig5.png)\n",
    "\n",
    "#### 2.1.3 Correspondence Pattern Recognition as a Clique Coverage Problem\n",
    "\n",
    "As was mentioned already before, the main problem of assigning different\n",
    "alignment sites to correspondence patterns is to decide about those cases where one site\n",
    "could be assigned to more than one patterns. Having shown how the data can be modeled\n",
    "in form of a network, we can rephrase the task of identifying correspondence patterns\n",
    "as a *network partitioning task* with the goal to split the network into non-overlapping\n",
    "sets of nodes. Given that our main criterion for a valid correspondence pattern is full \n",
    "compatibility among all alignment sites of a given partition, we can further specify the\n",
    "task as a clique partitioning task. A clique in a network is 'a maximal subset of the\n",
    "vertices [nodes] in an undirected network such that every member of the set is connected\n",
    "by an edge to every other' ([Newman 2010](http://bibliography.lingpy.org?key=Newman2010): 193). Demanding that sound correspondence\n",
    "patterns should form a clique of compatible nodes in the network of alignment sites is\n",
    "directly reflecting the basic practice of historical language comparison as outlined by\n",
    "Anttila (1972), according to which a further grouping of incompatible alignment sites by\n",
    "proposing a proto-form would require us to identify a phonetic environment that could\n",
    "show incompatible sites to be complementary.\n",
    "\n",
    "The minimum clique cover problem is a well-known problem in graph theory and\n",
    "computer science, although it is usually more prominently discussed in form of its inverse\n",
    "problem, the graph coloring problem, which tries to assign different colors to all nodes\n",
    "in a graph which are directly connected ([Hetland 2010](http://bibliography.lingpy.org?key=Hetland2010): 276). While the problem is\n",
    "generally known to be NP-hard (ibid.), fast approximate solutions like the Welsh-Powell\n",
    "algorithm ([Welsh and Powell 1967](http://bibliography.lingpy.org?key=Welsh1967)) are available. Using approximate solutions seems\n",
    "to be appropriate for the task of correspondence pattern recognition, given that we do\n",
    "not (yet) have formal linguistic criteria to favor one clique cover over another.\n",
    "\n",
    "### 2.2 A Method for Correspondence Pattern Recognition\n",
    "\n",
    "#### 2.2.1 General Workflow\n",
    "\n",
    "The general workflow underlying the method for automatic correspondence pattern\n",
    "recognition can be divided into five different stages. Starting from a multilingual wordlist\n",
    "in which translations for a concept list are provided in form of phonetic transcriptions\n",
    "for the languages under investigation, the words in the same semantic slot are manually\n",
    "or automatically searched for cognates (A) and (again manually or automatically)\n",
    "phonetically aligned (B). The alignment sites are then used to construct an alignment site\n",
    "network in which edges are drawn between compatible sites (C). The alignment sites are\n",
    "then partitioned into distinct non-overlapping subsets using an approximate algorithm\n",
    "for the minimum clique cover problem (D). In a final step, potential correspondence\n",
    "patterns are extracted from the non-overlapping subsets, and all individual alignment\n",
    "sites are assigned to those patterns with which they are compatible (E). While there are\n",
    "both standard algorithms and annotation frameworks for stages (A) and (B), the major\n",
    "contribution of this paper is to provide the algorithms for stages (C), (D), and (E). The\n",
    "workflow is further illustrated in the following figure. In the following sections, I will provide more\n",
    "detailed explanations on the different stages.\n",
    "\n",
    "![img](img/s10-fig6.png)\n",
    "\n",
    "#### 2.2.2 Input Format\n",
    "\n",
    "The input format follows the general input format used in LingPy and EDICTOR. \n",
    "In addition to the generally needed information on the identifier of each word (ID),\n",
    "on the language (DOCULECT), the concept or elicitation gloss (CONCEPT), the\n",
    "(not necessarily required) orthographic form (FORM), and the phonetic transcription\n",
    "provided in space-segmented form (TOKENS), the method requires information on the\n",
    "type of sound (consonant or vowel, STRUCTURE), the cognate set (COGID), and the\n",
    "alignment (ALIGNMENT). This is illustrated in the following table.\n",
    "\n",
    "![img](img/s10-tab3.png)\n",
    "\n",
    "#### 2.2.3 Cognate Detection and Phonetic Alignment\n",
    "\n",
    "Given that the method is implemented in form of a plugin for the LingPy library, all\n",
    "cognate detection and phonetic alignment methods offered in LingPy are also available\n",
    "for the approach and have been tested. The automatic methods for cognate detection and phonetic alignments, however, are\n",
    "not necessarily needed in order to apply the automatic method for correspondence pattern\n",
    "recognition. Alternatively, users can prepare their data with help of the EDICTOR tool\n",
    "users both to annotate cognates and alignments from scratch or to refine cognate sets\n",
    "and alignments that have been derived from automatic approaches.\n",
    "\n",
    "#### 2.2.4 Correspondence Pattern Recognition\n",
    "\n",
    "The method for correspondence pattern recognition consists of three stages (C-E in our\n",
    "general workflow). It starts with the reconstruction of an alignment site network in which\n",
    "each node represents a unique alignment site, and links between alignments sites are\n",
    "drawn if the sites are compatible, following the criterion for site compatibility outlined\n",
    "in Section 3.1 (C). It then uses a greedy algorithm to compute an approximate minimal\n",
    "clique cover of the network (D). All partitions proposed in stage (D) qualify as potentially\n",
    "valid correspondence patterns of our data. But the individual alignment sites in a given\n",
    "dataset may as well be compatible with more than one correspondence pattern. For this\n",
    "reason, the method iterates again over all alignment sites in the data and checks with\n",
    "which of the correspondence patterns inferred in stage (D) they are compatible. This\n",
    "procedure yields a (potentially) fuzzy assignment of each alignment site to at least one\n",
    "but potentially more different sound correspondence patterns (E). By further weighting\n",
    "and sorting the fuzzy patterns to which a given site has been assigned, the number of\n",
    "fuzzy alignment sites can be further reduced.\n",
    "\n",
    "The clique cover algorithm consists of two steps. In a first step, the data is sorted,\n",
    "using a customized variant of the Quicksort algorithm ([Hoare 1962](http://bibliography.lingpy.org?key=Hoare1962)), which seeks to\n",
    "sort patterns according to compatibility and similarity. By iterating over the sorted\n",
    "patterns, all compatible patterns are assigned to the same cluster in this first pass, which\n",
    "provides a first very rough partition of the network. While this procedure is by no means\n",
    "perfect, it has the advantage of detecting major signals in the data very quickly. For this\n",
    "reason, it has also been introduced into the EDICTOR tool, where a more\n",
    "refined method addressing the clique cover problem could not be used, due to the typical\n",
    "limitations of JavaScript running on client-side.\n",
    "\n",
    "In a second step, an inverse version of the Welsh-Powell algorithm for graph\n",
    "coloring (Welsh and Powell 1967) is employed. This algorithm starts from sorting all\n",
    "existing partitions by size, beginning with the largest partitions. It then consecutively\n",
    "compares the currently largest partition with all other partitions, merging those which\n",
    "are compatible with each other, and keeping the incompatible partitions in the queue.\n",
    "The algorithm stops, once all partitions have been visited and compared against the\n",
    "remaining partitions.\n",
    "\n",
    "The figure below gives an artificial example that illustrates how the basic method infers the\n",
    "clique cover. Starting from the data in (A), the method assembles patterns A and B in (B)\n",
    "and computes their pattern, thereby retaining the non-missing data for each language in\n",
    "the pattern as the representative value. Having added C and D in this fashion in steps (C)\n",
    "and (D), the remaining three alignment sites, E-G are merged to form a new partition,\n",
    "accordingly, in steps (E) and (F).\n",
    "\n",
    "![img](img/s10-fig8.png)\n",
    "\n",
    "In the final stage of assigning alignment sites to correspondence patterns, our method\n",
    "first assembles all correspondence patterns inferred from the greedy clique cover analysis\n",
    "and then iterates over all alignment sites, checking again whether they are compatible\n",
    "with a given pattern or not. Since alignment sites may suffer from missing data, their\n",
    "assignment is not always unambiguous. The example alignment from above, for\n",
    "example, would yield two general correspondence patterns, namely u-u-u-au vs. u-u-u-u.\n",
    "While the assignment of the alignment sites A and C in the figure would be unambiguous,\n",
    "the sites E and F would be assigned to both patterns, since, judging from the data, we\n",
    "could not tell what correspondence pattern they represent in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 Automatic Correspondence Pattern Recognition with LingPy\n",
    "\n",
    "The correspondence pattern algorithm is currently only available as a plugin for LingPy which can be downloaded from [the open science framework](https://osf.io/mbzsj/?view_only=\n",
    "b7cbceac46da4f0ab7f7a40c2f457ada). To install the plugin, one needs to make sure to have all dependencies installed (which are the same as the ones needed by LingPy), and then install the package by unpacking it and runnin the typical ```sudo python setup.py develop``` routine. The package itself is called `lingrex`, and our main module of this package is the `copar` module (=*Correspondence Pattern Recognition*). The code runs both with full and with partial cognates, and there are multiple tweaks to work with the data. In the following, however, I will restrict the demonstration to two examples, were we first align known cognates in a Germanic dataset and then investigate correspondence patterns, and then align known partial cognates in a Chinese dataset. \n",
    "\n",
    "### 3.1 Correspondence Pattern Analysis with Full Cognates\n",
    "\n",
    "We start by loading the data (file `S10-GER.tsv`) and carrying out an automatic alignment analysis, as illustrated in an earlier session. The concrete method we use to create the alignments is less relevant for us in this context, so we just use the defaults offered by LingPy. This analysis could also be carried out in the correspondence pattern library itself, since the relevant class the `CoPaR` class directly inherits from the `Alignments` class of LingPy. But to stick more clearly to the original workflow, we carry out the alignments now with help of the normal `Alignments` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lingpy import *\n",
    "alms = Alignments('../data/S10-GER.tsv', ref='cogid')\n",
    "alms.align()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To be able to carry out our correspondence pattern analysis on this data, we now need to add a dummy `STRUCTURE` column. This is a dummy column in our example, because we want to analyze *all* correspondence patterns, and not restrict the analysis to certain contexts. In many situations, however, it may be useful to split the data when carrying out the analysis and only investigate, for example, the patterns on vowels or the patterns on consonants. For this reason, the `copar` package expects a column name as input that indicates in which part of the input data the table with the structural representation can be found. By \"structure\" we mean here nothing else than an additional column that lists for each segment (column `TOKENS`) its respective \"class\", which can be the same class for all segments (and would enable a complete comparison of all patterns against each other), or a very fine-grained model, depending on the knowledge of the researcher about a particular language family. \n",
    "\n",
    "The `lingrex` package allows us to carry out these calculation from within the class, so we can now already start loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lingrex.copar import CoPaR\n",
    "cop = CoPaR(alms, ref='cogid', segments='tokens', alignment='alignment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Adding structure (in our case: the dummy structure that allows us to cluster all elements against all, regardless if they are vowels or consonants) is easily done with help fo the `add_structure` method. This method currently supports two models: \"cv\" (split into vowel, consonant, and tone) and \"c\" (disregard all class distinctions). The resulting representation of the `TOKENS` in the dataset is added to a new column `structure=\"keyword\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a     l\n",
      "    c     c\n",
      "\n",
      "   ɔː     l\n",
      "    c     c\n",
      "\n",
      "   æˀ     l\n",
      "    c     c\n",
      "\n",
      "    a    lː\n",
      "    c     c\n",
      "\n",
      "    a   t͡l     i     r\n",
      "    c     c     c     c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cop.add_structure(model='c', structure='structure')\n",
    "for i in [x for x in cop][:5]:\n",
    "    print(' '.join([x.rjust(5, ' ') for x in cop[i, 'tokens']]))\n",
    "    print(' '.join([x.rjust(5, ' ') for x in cop[i, 'structure'].split()]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can now run the analysis, by 1) getting the sites (`CoPaR.get_sites`), 2) sorting the patterns (`CoPaR.sort_patterns`), and then `CoPaR.cluster_patterns`. For the first function we need to inform the function which \"structure\" we have chosen as base representation. Don't forget to specify the structure-keyword! The column, where our structural (phonotactic) representation of the data resides is named \"structure\" now, so we need to indicate this. Note also that we set the keyword `minrefs` to 2, which means that we basically except all alignment sites (`minrefs` referst to `minimal number of reflexes`). You can use this keyword to yield more conservative scores that, however, also may exclude many alignments from the beginning, prior to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "patterns, sites = cop.get_sites(pos='c', structure='structure', minrefs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us first inspect our sites object, which is an `OrderedDict`, and thus behaves as a normal Python dictionary. The keys consist of tuples of integers, the first indicating the cognate identifier (our `COGID` in the data), and the second the concrete position in the alignment, using Python indices, thus starting from 0 to mark the position 1. If we know a certain cognate set, we can query the corresponding alignment site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan\tDut\tEng\tGer\tIce\tNor\tSwe\n",
      "---\t---\t---\t---\t---\t---\t---\n",
      "-\tʋ\tʋ\tv\t-\tØ\tØ\n",
      "o\tɔ\tɜː\tʊ\tɔ\tØ\tØ\n",
      "ʁˀ\tr\tr\tr\tr\tØ\tØ\n",
      "m\tm\tm\tm\tm\tØ\tØ\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join([x[:3] for x in cop.cols]))\n",
    "print('\\t'.join(['---' for x in cop.cols]))\n",
    "for i in range(4):\n",
    "    print('\\t'.join(sites[181, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From our cognate set for \"worm\", we can see clearly why we need two gap symbols, the classical `-` for alignments, pointing to zero-correspondences in the data, and the new gap symbol `Ø` pointing to missing data (here for Norwegian and Swedish, where the word for \"worm\" has been replaced by non-cognate words).\n",
    "\n",
    "We can now follow the basic procedure of the workflow and sort the patterns, as mentioned above. We derive a first set of patterns by using the `sort_patterns` method of the `CoPaR` class (and count, how many patterns the algorithm finds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n"
     ]
    }
   ],
   "source": [
    "patterns = cop.sort_patterns()\n",
    "print(len(patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The structure of the patterns object is a dictionary in which the pattern is now the key, and the values are lists of the tuples indicating the index of a given alignment site (cognate identifier plus the position in the alignment). By counting the size of the list of values, we can see, how many patterns are unique in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "print(len([p for p in patterns.values() if len(p) > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see whether the improved method for clustering yields more promising results than the simple sorting approach, let's now cluster the patterns with the Baum-Welsh algorithm. Here, we specify a number of iterations, as the algorithm may fail to cluster all patterns sufficiently during the first run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "cop.cluster_patterns(iterations=2)\n",
    "print(len(cop.clusters), len([p for p in cop.clusters.values() if len(p) > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that this has increased the number of non-unique patterns to quite some degree. 108 non-unique patterns for such a small dataset may still seem to be surprising, but since correspondence patterns have not yet been thoroughly investigated from a quantitative perspective, it may well be \"normal\", given the multiple possibilities where secondary change can occur that my disturb a given pattern. As a final example, we now assign patterns to fuzzy clusters and compute the fuzziness of our data. We also refine the patterns by adding each pattern to only one cluster, during which we take the information on fuzziness into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 312 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "cop.sites_to_pattern()\n",
    "cop.refine_sites()\n",
    "print('{0:.2f}'.format(cop.fuzziness()), len(cop.clusters), len([p for p in cop.clusters.values() if len(p) > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that the fuzziness for this dataset is not very high, which is promising, as it means that our 100 (we further reduced the number of clusters needed for our 313 different patterns) patterns are potentially rather regular instances of sound correspondences. We can now finally export these results to a text file that presents all patterns in spreadsheet form. Furthermore, we can export the wordlist format of the data, in which the patterns are given in a specific column, so that the data can be displayed in EDICTOR. For this, we first call the `add_patterns` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cop.write_patterns('../data/S10-patterns-out.tsv')\n",
    "cop.add_patterns(ref='patterns')\n",
    "cop.output('tsv', filename='../data/S10-wl-with-patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following table shows the basic structure of our file `S10-patterns-out.tsv`:\n",
    "\n",
    "ID | FREQUENCY | Danish | Dutch | English | German | Icelandic | Norwegian | Swedish | COGIDS | CONCEPTS\n",
    "--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
    "1 | 4 | g | x | ɡ | ɡ | k | g | g | 13:0, 56:0, 57:0, 150:0 | big / good / green / walk (V)\n",
    "2 | 4 | g | - | - | - | k | k | k | 2:2, 4:3, 51:3, 180:3 | ashes / bark / fish / worm\n",
    "3 | 2 | g | Ø | Ø | Ø | c | Ø | - | 28:1, 117:1 | cloud / skin\n",
    "4 | 1 | g | k | k | k | k | Ø | kː | 36:4 | drink (V)\n",
    "5 | 1 | g | Ø | Ø | Ø | c | k | kː | 99:1 | not\n",
    "6 | 1 | g | - | ɡ | - | kː | g | gː | 44:1 | egg\n",
    "7 | 1 | g | x | j | ɡ | k | g | g | 161:0 | yellow\n",
    "8 | 1 | g | x | ɡ | ɡ | c | j | j | 55:0 | give (V)\n",
    "9 | 1 | g | - | - | - | - | k | - | 101:5 | person\n",
    "10 | 1 | g | ɣ | - | ɡ | cː | g | gː | 74:2 | lie (V)\n",
    "\n",
    "What we can see from the table is that it infers all regular and irregular cases of correspondence patterns for each language as inferred by the algorithm. To facilitate inspection, entries are sorted according to the first language in the list, but if users select one language as their \"proto-language\", this language will be used for sorting. In our case, we can see that there is good evidence for the first two patterns, corresponding to 8 cognate sets, while the rest of the alignment sites and patterns are less convincing, given that they contain many gaps or occur only one time. This tutorial is not the place to discuss the nature of these findings (whether they are artifacts of the data or reflecting interesting aspects of sound change), but we can easily see that patterns occur with different frequencies in our data.\n",
    "\n",
    "### 3.2 Correspondence Pattern Analysis with Partial Cognates\n",
    "\n",
    "For the illustration of correspondence pattern detection with partial cognates, we will use the file `S10-BAI.tsv`, a file showing examples from the Bai dialects. This file has been very carefully segmented morphologically. Note that without the explicit segmentation, the `CoPaR` method usually throws an error. It is therefore very important to check that the data is carefully segmented and aligned, especially when analysing data from manual analyses.\n",
    "\n",
    "We start again by preparing the data, but this time, we carry out an automatic cognate detection analysis first, since our data does not contain information on cognacy so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "# load partial cognate detection tool\n",
    "from lingpy.compare.partial import Partial\n",
    "part = Partial('../data/S10-BAI.tsv')\n",
    "part.partial_cluster(method='sca', cluster_method='upgma', threshold=0.45, ref='cogids')\n",
    "\n",
    "# pass to alignment algorithm and align the data\n",
    "alms = Alignments(part, ref='cogids', fuzzy=True)\n",
    "alms.align()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now carry out the correspondence detection method. In contrast to the previous run, we will reduce the number of explanations (users can refer back to the correspondence pattern detection for full cognates for questions on the details). But in contrast to the previous examples, we will now reduce the analysis to consonants by adding \"cv\"-structures to the data instead of \"c\"-structures, where each segment is represented with a \"c\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "cop = CoPaR(alms, ref='cogids', segments='tokens')\n",
    "cop.add_structure(model='cv', structure='structure')\n",
    "patterns, sites = cop.get_sites(structure='structure', pos='c')\n",
    "patterns = cop.sort_patterns()\n",
    "print(len(patterns), len([p for p in patterns.values() if len(p) == 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are fewer patterns in the data than observed for the case for Germanic languages. This, however, is not due to a decreased diversity of the dialects (which is also evident, however), but more due to the fact that we restrict the analysis to consonants now, ignoring all vowels. \n",
    "\n",
    "In the last step, we follow the same approach as for full cognates in clustering the patterns fully and writing them to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustered 1: 156 72\n",
      "clustered 2: 155 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "cop.cluster_patterns()\n",
    "print('clustered 1:', len(cop.clusters), len([p for p in cop.clusters.values() if len(p) > 1]))\n",
    "cop.sites_to_pattern()\n",
    "cop.refine_sites()\n",
    "cop.write_patterns('../data/S10-pat-bai.tsv')\n",
    "cop.add_patterns(ref='patterns')\n",
    "cop.output('tsv', filename='../data/S10-bai-with-patterns')\n",
    "print('clustered 2:', len(cop.clusters), len([p for p in cop.clusters.values() if len(p) > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final algorithmic enhancement has again led to a reduction of our clusters (from 73 to 62). \n",
    "Looking at the data, however, shows that we are facing quite some degree of diversitty here, as a look on some counterparts of *n* in Dashi Bai reveals.\n",
    "\n",
    "ID | FREQUENCY | Dashi | Ega | Enqi | Gongxing | Jinman | Jinxing | Mazhelong | Tuolo | Zhoucheng | COGIDS | CONCEPTS\n",
    "--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
    "1 | 11 | n | - | - | n | - | n | n | n | - | 126:3, 176:2, 274:3, 282:2, 313:2, 332:3, 353:2, 383:2, 448:2, 605:3, 614:2 | ear / heart / moon / near / new / nose / person / round / sun / wind / woman\n",
    "2 | 4 | n | ŋ | Ø | n | Ø | - | n | n | - | 73:2, 413:0, 537:2, 589:2 | burn / sleep / warm / who\n",
    "3 | 3 | n | n | n | n | n | n | n | n | n | 484:0, 502:0, 510:0 | that / this / thou\n",
    "4 | 2 | n | ŋ | ŋ | ŋ | ŋ | Ø | Ø | ŋ | ŋ | 84:0, 145:0 | cloud / fish\n",
    "5 | 1 | n | Ø | Ø | Ø | Ø | n | n | - | - | 138:3 | far\n",
    "6 | 1 | n | - | - | n | - | n | - | n | - | 132:2 | egg\n",
    "7 | 1 | n | - | - | - | - | n | n | n | - | 423:2 | smoke\n",
    "8 | 1 | n | - | Ø | ŋ | Ø | n | - | Ø | Ø | 110:2 | ear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Correspondence Pattern Inspection with EDICTOR\n",
    "\n",
    "EDICTOR offers two ways to inspect correspondence patterns which are both provided along with the `ANALYZE->Correspondence Patterns` panel of the tool. For the first analysis, EDICTOR employs the simple sorting algorithm to arrive at a rough clustering of the data. This is convenient for users interested in learning to which degree their correspondence patterns are recurring frequently in their data. All that is needed to use this module are a column with the cognate identifiers (`COGIDS`), a column with segments (`TOKENS`), the language (`DOCULECT`) and the concept (`CONCEPT`), and, of course, the (correct) alignment (`ALIGNMENT`). Before loading the panel in the EDICTOR tool, a popup will ask the user to indicate the preference (full or partial cognates). Afterwards, the window displays all cognate sets in the data arranged by compatible alignment sites. The following screenshot is just an example for the Germanic data in which the preview is restricted to 10 rows:\n",
    "\n",
    "![img](img/s10-fig9.png)\n",
    "\n",
    "There are now several possibilities to analyze the data further. Users can change the threshold of reflexes that must be included in the data. After selecting, one needs to press the \"REFRESH\" button to make sure that the tool reloads the data. Users can also select specific patterns according to theri basic characteristics (derived from the sounds of the first language in the sample (again: if a proto-language is in the sample, users can specify this and will be given a convenient way to browse their data). Clicking on the cells in the COGNATES column will open the corresponding alignment, and clicking on the individual cells for each language will show the whole word under question. As a final information, the SIZE column indicates roughly, how \"filled\" a column is, although this score is only sub-optimal for the time being.\n",
    "\n",
    "A probably even more useful way to inspect correspondence patterns is to go through the analyses made by the Python algorithm, since these are exploring the data much more exhaustively. In order to do so, one only needs to have a file with the data fields (columns) as indicated above and an extra column that should be called `PATTERNS`. \n",
    "Since the format specifics are not straightforward to be manually edited, this file should be created with LingPy, as has been shown above. If this file is loaded and the patterns view is selected, EDICTOR will display the patterns inferred by the algorithm rather then inferring them itself. The advantage is a much more refined view on correspondence patterns that makes it very convenient for scholars to check how convincing their alignments and cognates are for a given dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Open Questions\n",
    "\n",
    "There are a couple of open questions:\n",
    "\n",
    "* how can the algorithm be refined?\n",
    "* could the algorithm for correspondence pattern recognitions, be used on other types of data, e.g., structural features, e.g., to identify instances of features that evolve together?\n",
    "* how can we conveniently modify EDICTOR to allow users to re-define patterns so that they can annotate data for correspondence patterns without relying on the computer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
