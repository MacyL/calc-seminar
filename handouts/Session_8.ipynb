{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring, Modeling, and Analysing Cognate Sets (Johann-Mattis List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Traditional Approaches to Cognate Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 The Comparative Method\n",
    "\n",
    "The comparative method, as the \"fundamental method\" for the identification of\n",
    "sound correspondences and the reconstruction of proto-languages, has many\n",
    "different definitions in the literature. A very popular description of the workflow can be found in [Ross and Durie (1996: 6f)](:bib:Ross1996):\n",
    "\n",
    "> 1. Determine on the strength of diagnostic evidence that a set of languages are genetically related, that is, that they constitute a 'family';\n",
    "> 2. Collect putative cognate sets for the family (both morphological paradigms and lexical items).\n",
    "> 3. Work out the sound correspondences from the cognate sets, putting 'irregular' cognate sets on one side;\n",
    "> 4. Reconstruct the protolanguage of the family as follows:\n",
    ">  1. Reconstruct the protophonology from the sound correspondences worked out in (3), using conventional wisdom regarding the directions of sound changes.\n",
    ">  2. Reconstruct protomorphemes (both morphological paradigms and lexical items) from the cognate sets collected in (2), using the protophonology reconstructed in (4a).\n",
    "> 5. Establish innovations (phonological, lexical, semantic, morphological, morphosyntactic) shared by groups of languages within the family relative to the reconstructed protolanguage.\n",
    "> 6. Tabulate the innovations established in (5) to arrive at an internal classification of the family, a 'family tree'.\n",
    "> 7. Construct an etymological dictionary, tracing borrowings, semantic change, and so forth, for the lexicon of the family (or of one language of the family).\n",
    "\n",
    "This workflow, however, is not always followed by all scientists, especially also since the tree building part (steps 5 and 6) are nowadays often replaced by the use of phylogenetic methods. For that reason,\n",
    "I see\n",
    "the core of the classical workflow of historical language comparison as shown on\n",
    "the figure on the right. The dashed lines\n",
    "indicate that each step of this workflow is\n",
    "iterative and interacts with other steps.\n",
    "\n",
    "![img](img/s8-cm.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Traditional Workflow\n",
    "\n",
    "In practice, people often claim that they follow the traditional workflow of the comparative method, but since this workflow is barely agreed-upon by scholars, it is difficult to determine what people do in concrete when they carry out the reconstruction of a language family.\n",
    "All we can do, in order to determine the most prototypical workflow followed by scholars, is basing this on our personal experience, that is, the practice that our colleagues follow usually. \n",
    "If we look at this practice, we can\n",
    "describe this procedure (at least the one, I witnessed myself) as follows:\n",
    "\n",
    "* Assemble a list of potential cognate sets.\n",
    "* Align the words in your cognate list.\n",
    "* Extract a list of potential sound correspondences from the alignments.\n",
    "* Improve the cognate list and the correspondence list by:\n",
    "  * Adding and removing correspondences from the correspondence list.\n",
    "  * Adding and removing cognates from the cognate list.\n",
    "* Stop, when the results are satisfying and ready for publication.\n",
    "\n",
    "While this procedure sounds rather systematic, it is clear that it contains many darker areas where it is not essentially clear what people could or should do. For example, as we have seen in last week's session, not all words are entirely alignable, but scholars still seem to align them in their heads or label them as cognates. This impliciteness is also one of the major reasons why automatic cognate detection is so difficult to automatize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 Automatic Cognate Detection\n",
    "\n",
    "The major challenges for automatic cognate detection lie in impliciteness of the general workflow. In the following, we will ignore these problems, resulting from partial cognacy, multi-morphemic words, and morphological change, and try to look at the task from the rather restricted perspective of finding all cognates in a multi-lingual wordlist for all words that have the same meaning. This restriction is often seen as less important by linguists, who claim that semantic change is so frequent that many of the cognates between a couple of languages have shifted their meanings and are therefore no longer detectable if we restrict our search to cognates with the same meaning. However, since we do not have a full-fledged theory of semantic change that we could apply when comparing cognate sets across different meanings, it is important to restrict the task in such a way that it can be automatically handled. Furthermore, all \"proofs\" that scholars presented in favor of their theory that most cognates contain shifted meanings are anecdotal, based on only a couple of examples of extreme semantic shift (compare [Szemérenyi 1970: 14f](:bib:Szemerenyi1970)), and it is not even clear whether an all-to-all search, in which all words are compared against each other cross-linguistically might not lead to unknown distortions and an explosion of lookalikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 Detection of Sound Correspondences\n",
    "\n",
    "Ignoring problems resulting from unalignable parts in words or sound change processes that cannot be represented straightforwardly in linear alignment algorithms, the detection of sound correspondences for a pair of languages can be easily derived from aligned cognate sets. A potentially regular sound correspondence could then just be treated as a frequently occurring match between a sound in language A and a sound in language B. \n",
    "\n",
    "Since we want to feed the information on sound correspondences, or, to be more precise, sound correspondence *probabilities*, for a given language pair to a computer algorithm, we need to find a way to measure the *strength* of our evidence. In order to do so, we can easily turn to sequence comparison in biology, where we find good ways to handle the problem of matching probabilities. Thus, in bioinformatics, it is important to compute the probability of correspondences in DNA and\n",
    "protein alignment. This is done by comparing an *attested* with an *expected distribution*.\n",
    "Transferred to linguistics, this means that we compare a list of corresponding sounds with a distribution which we would expect if the languages were not genetically related. In order\n",
    "to substantiate this, linguists usually show long lists of potential cognates, as shown in the\n",
    "list below:\n",
    "\n",
    "Meaning | Italian | French\n",
    "--- | --- | ---\n",
    "\"square\" | pjaʦːa | plas\n",
    "\"feather\" | pjuma | plym \n",
    "\"flat\" | pjano | plɑ\n",
    "\"tear\" | lakrima | laʀm\n",
    "\"tongue\" | liŋgwa | lɑ̃ɡ\n",
    "\"moon\" | luna | lyn\n",
    "\n",
    "However, in the end, it is not only lists of words which are interesting for us, but lists of\n",
    "aligned words. Without alignments, we cannot properly construct our list of sound correspondences, neither manually, nor computationally.\n",
    "\n",
    "![img](img/s8-alms.png)\n",
    "\n",
    "While approaches in biology usually stop here, counting the attested distributions of sound correspondences in the data and comparing them against expected distributions which they derive from combinatoric considerations involving the frequency of characters, the table should make it clear why we cannot use simple combinatorics to derive our expected distributions (and therefor the correspondence probabilities) from phonetic alignments for our data: sound change is often context-restricted, occurring only in certain position of a given word. As a result, there are numerous sound correspondences which we can only find attested in certain positions of words. For example, the correspondence in our example between French *l* and Italian has at least to different types in our data:\n",
    "\n",
    "Italian | French\n",
    "--- | ---\n",
    "j | l \n",
    "l | l \n",
    "\n",
    "If we look at the position of the *l* in French in our *l*-*j* correspondence, we can easily see that this is related to those cases where an *l* is preceded by *p* (in our examples), or more generally, a plosive or a fricative (if we look at more examples for this correspondence). This means essentially that we cannot follow the simplfying approach that handles correspondence probabilities as independent, as it is assumed in biological approaches. What we need to do instead is finding a way to handle context-dependent probabilities.\n",
    "\n",
    "While we have not yet found a proper complete solution for this problem, the usage of prosodic information added to the words as a second \"tier\" of representation that encodes each sound both with respect to its phonetics (the normal IPA representation) *and* its prosodic prosodic context (inspired by [Geisler 1992](:bib:Geisler1992), see [List (2014: 119-133)](:bib:List2014)), can help us to approximate at least the most general cases and problems. \n",
    "While prosodic context originally serves the purpose of determining the *weight* of a given sound segment derived from a *hierarchy of strength*, with strong elements being less likely to undergo sound change, and weak elements being more likely to change or be lost (Geisler 1992), it can also be used to add contextual information to correspondence probability analyses within a multi-tiered sequence representation (which we will detail out in Session 9 of this seminar). Instead of distinguishing only one *l* in French, we can easily claim that there are two different *l* instances in our sample French distinguishing the *l* that occurs word-initially, and the *l* that occurs in the position of a prosodic environment of *ascending sonority*. How this can be done is illustrated in the following for the Bulgarian *jabəlka* \"apple\".\n",
    "\n",
    "![img](img/s8-multi-tiers.png)\n",
    "\n",
    "If we apply this idea to our French-Italian example, and count the matches in our alignments for the tuples derived from the two tiers, we can easily see that the scores for our sound correspondence matches become much more meaningful now, with two *l* instances being differentiated in French, one corresponding to *j* in Italian, and one to *l*.\n",
    "\n",
    "![img](img/s8-corrs.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Cognate Set Partitioning\n",
    "\n",
    "Partitioning (also called *flat clustering*) is the process by which objects are divided into groups. If we talk about the Wú\n",
    "dialects in China, for example, we talk about a partitioning of the Chinese dialects into one\n",
    "group which we call Wú 吴. Cognate detection is also a partitioning procedure, as we divide\n",
    "words into groups, and we assume that words inside a group go back to a common ancestor. The words German *Zahn* [ʦaːn], Italian *dente* [dɛnte], Dutch *tand* [tand], Russian *zub*\n",
    "[zup], und English *tooth* [tʊːθ] (all meaning \"tooth\") can be clustered into different groups.\n",
    "Some go back to Proto-Indo-European `*`deh₃nt- \"tooth\" (*Zahn*, *dente*, *tand*, and *tooth*),\n",
    "and one goes back to Proto-Indo-European `*`ǵombh-o- “(finger)nail” (*zub*) ([Derksen 2008: 549](:bib:Derksen2008)).\n",
    "\n",
    "Automatic partitioning usually requires that a *distance matrix* between the elements is passed to the partitioning algorithm. Most algorithms construct some kind of a hierarchy (a tree or a hierarchical cluster) of all elements, in which the elements are arranged according to their distance (with the closest elements being direct neighbors on the tree, etc.). The partitioning procedure then cuts off certain branches of the the tree in order to arrive at a certain number of groups. For that reason, all partitioning algorithms need some kind of threshold as input, which determines when exactly a couple of elements are similar enough to be placed in the same partition. [List et al. (2017)](:bib:List2017c) further distinguish tree-based from network-based clustering methods, as illustrated in the following figure, taken from their paper, but alghough network-based methods do not directly expose the underlying hierarchical clustering to the user, it is clear that most approaches still produce the hierarchy, albeit internally.\n",
    "\n",
    "![img](img/s8-cluster.png)\n",
    "\n",
    "Automatic clustering has the advantage that the evidence which may be missing when\n",
    "comparing only one language pair, can be backed up by additional evidence. This nicely\n",
    "accounts for the use of cumulative evidence ([Sturtevant 1920: 11](:bib:Sturtevant1920), which is a fundamental\n",
    "aspect of the comparative methods for historical language comparison. We can see this in the following matrix that shows the computed SCA distances for the five words for \"tooth\" mentioned above: if it was only for the pairwise information, we might end up judging Russian *zub* to be cognate with German *Zahn*, while the comparison with French, Dutch, and English shows that *zup* is not likely to belong to the cluster of the cognate words in German, French, Dutch, and English.\n",
    "\n",
    "There are many different methods available by which the distance matrix could be calculated. The different cognate detection methods offered by LingPy mostly differ in this respect, while the partitioning procedure is the same in most approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 LexStat Algorithm\n",
    "\n",
    "Below is the workflow of the LexStat method for automatic cognate detection (List 2012a).\n",
    "This method cumulates the aforementioned ideas for automatic cognate detection and\n",
    "assigns them to a common framework which comes close to the basic ideas of the *comparative method*. Phonetic alignment plays a two-fold role: first it is used as initial heuristic\n",
    "to find the best candidates when being used to analyse multiple languages. Second, it is\n",
    "used as final procedure to infer the distances between all strings which are then fed to a\n",
    "cluster algorithm that finally partitions the data into groups of supposedly cognate words.\n",
    "\n",
    "![img](img/s8-lexstat.png)\n",
    "\n",
    "The phonetic alignment algorithm is based on sound classes. It does not align phonetic\n",
    "sequences directly, but rather modifies IPA characters to the simpler sound classes first,\n",
    "and later converts them back, as illustrated in the figure below.\n",
    "\n",
    "![img](img/s8-sca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 Handling Cognates in LingPy\n",
    "\n",
    "With our tutorial on LingPy being published soon ([List et al. 2018](http://lingulist.de/documents/papers/list-et-al-2018-lingpy-tutorial.pdf), we have now finally an authoritative source illustrating how cognates can and should be computed with help of LingPy. Instead of repeating the full tutorial here, which would go too much beyond this session, I will just pick out a few of the examples, but recommend all those who are interested to learn more about cognate detection with LingPy (especially the LexStat method), to turn to both our paper draft and the concrete [IPython notebook tutorial](https://github.com/lingpy/lingpy-tutorial), as it offers a much more profound overview on all methods available with LingPy.\n",
    "\n",
    "### 3.1 Loading a Dataset with LingPy into the LexStat Class\n",
    "\n",
    "Let's start with our toy dataset ```S08_east-polynesian.tsv``` and load this into the ```LexStat``` class of LingPy, the basic class that handles cognate detection methods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lingpy import *\n",
    "lex = LexStat('../data/S08_east-polynesian.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check, how many concepts we find in our list, and how many languages we have, we can just query the basic parameters of word list objects: their \"width\" (number of languages), their \"height\" (number of concepts), and their \"length\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts: 210\n",
      "Lanuages: 10\n",
      "Words: 2427\n"
     ]
    }
   ],
   "source": [
    "print('Concepts: {0}\\nLanuages: {1}\\nWords: {2}'.format(lex.height, lex.width, len(lex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computing Cognates: Phenetic Methods\n",
    "\n",
    "The \"phenetic\" methods are those that derive their distance scores for words without taking any further information on sound correspondences or so-called language-specific similarities into account. LingPy offers three basic methods in this regard: the Consonant-Class method (```turchin```, [Turchin et al. 2010](:bib:Turchin2010)), the SCA method (```sca```), and the *normalized edit distance* method (NED, ```edit-dist```). When computing these clusters, LingPy iterates over each concept slot in the wordlist, computes the pairwise word distances, stores them in a matrix, and then partitions the words into cognate sets using one of four potential partition methods (```cluster_method```: ```upgma```, ```single```, ```complete```, ```infomap```). For the last method, it is important that the [igraph](http://igraph.org) library is installed on the system. Since ```upgma``` is the most frequently used method, we will restrict our example on this cluster method. Note that the user also has to choose a threshold, for which we use those which have performed best on alternative datasets for the respective method in the past. The resulting data is stored in a new column added to the wordlist file, whose name can be modified with help of the ```ref``` (=REFERENCE_COLUMN) keyword. All in all, this leaves us with the following code lines to test the NED and the SCA methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "lex.cluster(method='sca', threshold=0.45, ref='scaid', cluster_method='upgma')\n",
    "lex.cluster(method='edit-dist', threshold=0.45, ref='editid', cluster_method='upgma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have excluded the ```turchin``` method here, since this method does not need any additional parameters, not even a threshold, since it only compares the first two consonant classes (following the sound class schema of Dolgopolsky) for each word. We can thus compute it without the threshold and the cluster-method as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "lex.cluster(method='turchin', ref='turchinid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access the data, we can either download the cognate sets and inspect them in EDICTOR (we will do this later), or we can do this quickly from within LingPy. For example, we can inspect and compare the cognate decisions that the algorithms make for the concept \"Eight\" in the data. In order to do so, we first extract all IDs for the concept \"Eight\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5614, 1853, 725, 3297, 1595, 4395, 5101, 3076, 1169, 4592]\n"
     ]
    }
   ],
   "source": [
    "idxs = lex.get_list(row='Eight', flat=True)\n",
    "print(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these IDs, which are stored in the ```idxs``` list, we can now infer all method we need by querying the ```LexStat``` object, which behaves just as a normal LingPy wordlist. We print out language (```doculect```), word form (```ipa```), and the cognate judgments for our three methods. The cognate identifiers indicate for all words, whether they are cognate or not. If they are cognate, they are given the same identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hawaiian        walu      1    1    1\n",
      "Mangareva       varu      1    1    2\n",
      "Maori           waru      1    1    1\n",
      "North_Marquesan vaʔu      4    1    2\n",
      "Rapanui         vaʔu      4    1    2\n",
      "Ra’ivavae       vaɢu      6    1    2\n",
      "Rurutuan        vaʔu      4    1    2\n",
      "Sikaiana        valu      1    1    2\n",
      "Tahitian        vaʔu      4    1    2\n",
      "Tuamotuan       varu      1    1    2\n"
     ]
    }
   ],
   "source": [
    "for idx in idxs:\n",
    "    print('{0:15} {1:6} {2:4} {3:4} {4:4}'.format(\n",
    "        lex[idx, 'doculect'],\n",
    "        lex[idx, 'ipa'],\n",
    "        lex[idx, 'turchinid'],\n",
    "        lex[idx, 'scaid'],\n",
    "        lex[idx, 'editid'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3\n",
    "\n",
    "In order to compute the cognate sets with help of the LexStat algorith, we need to compute pairwise language scores first. These scores indicate for each tuple of sound class, language, and prosodic context the probability the probability of being \"regular\". Regularity is here defined in logarithmic scores, that means: if the score is negative, it is rather unlikely that the correspondence is \"regular\", and the higher it is, the better. These scores are pre-computed by using a computationally rather expensive procedure by which all word lists are shuffled (a so-called permutation approach) and words from different concepts are aligned with each other. The segments which are align here are compared with the ones that are found when comparing words in the same concept slot. In the end, the two frequencies are combined to form one logarithmic score which can then be used directly in the alignment algorithms, from where it is converted to a distance score that is fed to the clustering algorithm.\n",
    "\n",
    "The first step important for LexStat is to compute this language-specific \"scorer\", using the ```get_scorer``` method. The parameters are here:\n",
    "\n",
    "* ```runs```: the number of permutations (defaults to 1000, but serious approaches should use 10000)\n",
    "* ```ratio```: thee ratio of \"pure\" scores derived from pairwise language-specific matches only and the normal SCA scores (some kind of smoothing to allow for consistent results when using small wordlists), usually set to 2:1\n",
    "* ```threshold```: the threshold which is used to derive the attested distribution (we usually don't want to derive the attested distribution from all alignments, but only the rather good ones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 13:51:39,888 [WARNING] A different scoring function has already been calculated, overwriting previous settings.\n",
      "                                                                                    \r"
     ]
    }
   ],
   "source": [
    "lex.get_scorer(runs=1000, ratio=(1,1), threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once computed, we can compute the LexStat cognates in the same way in which we computed the cognates with SCA and edit distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column <lexstatid> already exists, do you want to override? [y/N] yes\n",
      "Hawaiian        walu      1    1    1    1\n",
      "Mangareva       varu      1    1    2    1\n",
      "Maori           waru      1    1    1    1\n",
      "North_Marquesan vaʔu      4    1    2    1\n",
      "Rapanui         vaʔu      4    1    2    1\n",
      "Ra’ivavae       vaɢu      6    1    2    1\n",
      "Rurutuan        vaʔu      4    1    2    1\n",
      "Sikaiana        valu      1    1    2    1\n",
      "Tahitian        vaʔu      4    1    2    1\n",
      "Tuamotuan       varu      1    1    2    1\n"
     ]
    }
   ],
   "source": [
    "lex.cluster(method='lexstat', cluster_method='upgma', threshold=0.6, ref='lexstatid')\n",
    "\n",
    "for idx in idxs:\n",
    "    print('{0:15} {1:6} {2:4} {3:4} {4:4} {5:4}'.format(\n",
    "        lex[idx, 'doculect'],\n",
    "        lex[idx, 'ipa'],\n",
    "        lex[idx, 'turchinid'],\n",
    "        lex[idx, 'scaid'],\n",
    "        lex[idx, 'editid'],\n",
    "        lex[idx, 'lexstatid'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We output the data to TSV, in order to be able to inspect the data in EDICTOR. But before we do so, let's quickly also align all words automatically. In order to do so, we can simply pass the LexStat instance to the `Alignments` class of LingPy (we just need to indicate, which of the columns contains the cognate identifiers, using the ```ref``` keyword):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alm = Alignments(lex, ref='lexstatid')\n",
    "alm.align()\n",
    "alm.output('tsv', filename='../data/S08-aligned', ignore='all', prettify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4 Handling Cognates in EDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When loading the data in EDICTOR, in the same way in which we discussed this before, we can now systematically \n",
    "\n",
    "* revise the cognate decisions that have been made, \n",
    "* revise the alignments\n",
    "* inspect pairwise sound correspondences derived from the alignments\n",
    "* inspect sound correspondence patterns (not shown in this session, as it is part of the next session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to inspect the alignments as they have been created by LexStat, we first need to adjust the settings in EDICTOR, after loading the file, as EDICTOR displays by default only those cognate sets in the column ```COGID```, which are not based on our automatic computation. We do this by clicking on SETTINGS and replacing the ```COGID``` by ```LEXSTATID```, before clicking on REFRESH. One should also select the respective columns from the columns selection menu in EDICTOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"800\" height=\"400\" controls>\n",
       "  <source src=\"img/s8-lexstat-1.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s8-lexstat-1.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To edit the cognate sets, one can select two different approaches:\n",
    "\n",
    "* manual editing by filling the text fields in the column storing the cognate sets\n",
    "* guided editing using the cognate set editor (EDIT->COGNATE SETS)\n",
    "\n",
    "Manual editing is at times useful, especially if one just wants to prepare some dataset rather quickly, or the cognate set editor is stuck for some reasons. What users should know here is that inserting any non-numerical number into the field that has been selected as containing cognate sets in a given session (e.g., LEXSTATID in our example), will produce the lowest possible free cognate identifier. That means, by simply inserting some non-numeric value into all fields, scrolling down with arrow keys, will set all cognate judgments to \"zero\" by assigning all words to different cognate sets.\n",
    "\n",
    "When editor cognate sets using the editor, the basic procedure follows a slot-wise approach, although it is also possible to assign cognates for more than one slot (e.g., across different meanings). In order to do so, one has to use the meaning selection menu. \n",
    "\n",
    "When editing cognate sets, there are two major operations, namely one that *merges* different cognate sets into the same cognate set (```COMBINE```), and one that creates a *new* (```NEW```) cognate set for the selected words. These are the only operations needed, and when clicking at the right-most symbol in each cognate set displayed in the cognate edit panel, one can directly align the cognate sets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"800\" height=\"400\" controls>\n",
       "  <source src=\"img/s8-lexstat-2.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s8-lexstat-2.mp4\" type=\"video/mp4\">\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In addition to *editing* cognate sets, users can also *analyze* via the analyze mode (ANALYZE -> COGNATE SETS). This view essentially lists cognate sets per meaning slot in tabular form and allows users in this way to quickly search for gaps in their data, or for additional potentially interesting patterns (like shared cognates between two or more subgroups in the data). The cognate set view should be self-explaining, especially if users are ready to just play around with the EDICTOR a bit and test some of the general features that usually recur (double-click for sorting of columns, menus for sub-selection, clicking on elements and seeing what happens). In addition to this view, the ANALYZE->COGNATE SETS panel also offers to export the data to NEXUS, from which users can compute language trees and networks.\n",
    "\n",
    "The following, for example, is the SplitsTree computed from the NEXUS file we exported with help of EDICTOR (based on the \"normal\" expert cognates in the data):\n",
    "\n",
    "![img](img/s8-network.svg)\n",
    "\n",
    "When invoking the analysis mode, the user will have to choose between \"partial\" and \"full\" cognates as mode. In our case, our cognates are \"full\", but if users use partial cognates (discussed quickly in the next session), the \"partial\" mode needs to be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"800\" height=\"400\" controls>\n",
       "  <source src=\"img/s8-lexstat-3.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s8-lexstat-3.mp4\" type=\"video/mp4\">\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following screen video further shows, how we can create a network of the data with help of the exported NEXUS format loaded into the [SplitsTree](http://splitstree.org) software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"800\" height=\"400\" controls>\n",
       "  <source src=\"img/s8-lexstat-4.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" height=\"400\" controls>\n",
    "  <source src=\"img/s8-lexstat-4.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
