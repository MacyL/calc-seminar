{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multi-Tiered Sequence Representation (Cormac Anderson, Johann-Mattis List, and Tiago Tresoldi)\n",
    "\n",
    "## 1 Beyond Segments (Cormac Anderson)\n",
    "\n",
    "## 1.1 The Units of Phonological Representation\n",
    "\n",
    "Problem of segmentation in linguistics – sentence, phrase, word, phoneme.\n",
    "\n",
    "The acoustic signal is continuous – viewed in a spectrogram it is not a priori obvious where to segment the signal – \"render discrete what is continuous\".\n",
    "\n",
    "What are we trying to render discrete? What is being segmented?\n",
    "\n",
    "Idea of a basic-level prototype category in Cognitive Linguistics\n",
    "\n",
    "Context dependent\t\n",
    "* dog in the class of animals, insufficient at a dog show\n",
    "* tree as in \"I crashed my bicycle into a ______\", but for the master carver?\n",
    "\n",
    "Is the phoneme a basic-level prototype category?\n",
    "\n",
    "What would constitute good evidence? Glottogeny?\n",
    "\n",
    "Writing systems tend to develop as syllabaries (or logographic systems)\n",
    "\n",
    "Diffusion of alphabets an historical contingency\n",
    "\n",
    "Psychological evidence for cognitive primary of segment only for people who are already literate in an alphabetic script Jaeger (1980)\n",
    "\n",
    "Phoneme concept a \"phonetic hypostatisation of Roman letters\" (Firth 1948)\n",
    "\n",
    "Where does this leave us?\n",
    "\n",
    "## 1.2 Phonological Domains Above and Below the Segment\n",
    "\n",
    "Rather than assuming that phonological representation consists of discrete sequences of letter-sized chunks, we can think instead of features (in theory-neutral sense) operating across a domain\n",
    "\n",
    "This domain can be smaller than the level of a segment, or larger than it\n",
    "\n",
    "Think of the prototypical stop-vowel sequence: acoustically, is this just CV?\n",
    "\n",
    "Silence → release noise →consonant-vowel transition → vowel target\n",
    "\n",
    "Each of these phases are critical to the correct identification of functional contrast\n",
    "\n",
    "The presence of silence makes it a stop rather than a fricative, noise an oral rather than nasal stop, CV transition cues towards localisation of both consonant and vowel, etc.\n",
    "\n",
    "The domain of each of these is actually smaller than the segment, as classically understood, also known as subsegmental\n",
    "\n",
    "As well as being capable of subdivision, it has long been recognised that segments share features, and that phonological rules can target certain features: typically rules do not target phonemes, but rather classes of phonemes\n",
    "\n",
    "For example, vowel deletion rules (vowels not consonants); vowel insertion rules (consonants not vowels); spirantisation of stops (stops not fricatives); voicing of intervocalic voiced consonants (voiceless not voiced), etc.\n",
    "\n",
    "This fact alone militates against viewing the phoneme as a basic entity of phonological representation – can rather be seen as an emergent property, as a (relatively arbitrary) agglomeration of cotemporal and consecutive phonetic events\n",
    "\n",
    "There are also domains of operation above the level of the segment, some of which are known to classical phonological theory, e.g. syllable, foot, word\n",
    "\n",
    "Phenomena operating over larger domains, e.g. stress, tone, etc. also known as suprasegmental\n",
    "\n",
    "## 1.3 Abstractive Phonology\n",
    "\n",
    "Irish lenition\t\t\n",
    "* [p t k b d g] > [φ θ x β ð ɣ] /V_V (intervocalic spirantisation)\n",
    "* targets stops, i.e. feature [-son, -cont]; {ʔ}, {silence} etc.\n",
    "\n",
    "Irish fortition\t\t\n",
    "* [φφ θθ xx ββ ðð ɣɣ] > [pʰ tʰ kʰ b d g]\n",
    "* what is targeted here? can two [+cont] values make a [-cont]? possible to \t\t\t\twrite arbitrary rules, \n",
    "* clear here that stopness is a structural configuration\n",
    "* phonological gemination = phonetic occlusion\n",
    "\n",
    "Welsh lenition\t\t\n",
    "* [p t k] > [b d g]\n",
    "* targets voiceless, i.e. feature [-voice]; {H}, etc.\n",
    "\n",
    "Welsh fortition\t\n",
    "* [bb dd gg] > [p t k]\n",
    "* what is targeted here? can two [+voice] values make a [-voice]?\n",
    "* clear here that voicelessness is a structural configuration\n",
    "* phonological gemination = phonetic voicelessness\n",
    "\n",
    "Constant mismatches between phonological structure and phonetic outcomes – universal feature sets cannot capture this. Inference of rules requires analysis of phonological behaviour.\n",
    "\n",
    "* Turkish vowel harmony: suffixes agree with the root in (1) front-back quality of vowels, (2) rounded-unrounded quality of (only high) vowels:\n",
    "* Root vocalism: [i e y ø ɯ a u o]\n",
    "* Suffix vocalism (low): [e a]\n",
    "* Suffix vocalism (high): [i y ɯ u]\n",
    "\n",
    "* Firthian approach would abstract features from the relevant domain of operation (here the word). [front] and [back] are properties of the entire word, not of individual vowels (similarly [round]).\n",
    "* Specified root, underspecified suffixes, possible epenthetic /ə/\n",
    "* Similar approach by Moscow School of phonologists (Jakovlev, Reformatsky)\n",
    "* Advantages – simpler statement, coarticulation easily dealt with (velars and /l/)\n",
    "\n",
    "In Central Chadic languages, similar phenomena, but it is rather the root that is underspecified, affixes specified for [front-back] etc. (examples from Moloko: Bow 1997)\n",
    "\n",
    "* /mndzar/ 'see'\n",
    "* /n(a)-/ '1sg.'\n",
    "* /-am/W 'plural'\n",
    "* [nəməndzar] 'I see'\n",
    "* [nɔmʊndzʊrɔm] 'we see'\n",
    "\n",
    "Surface vocalism in such languages often similar to Turkish (8 vowels), but descriptive practice of 1-2 vowels, with [front-back] and [round] abstracted away from the entire domain\n",
    "\n",
    "Phonetic confirmation for this analysis (Pearce 2008) in that there is not reduction on the F2 axis\n",
    "Consonants also affected (mainly coronals)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 Preliminaries of Multi-Tiered Sequence Representation (Johann-Mattis List)\n",
    "\n",
    "### 2.1 Pseudo Words and Phonological Rules\n",
    "\n",
    "When dealing with questions of historical phonology (but also synchronic phonology), linguists often forget to make clear to themselves what questions they want to investigate in the end. Apart from the large pool of potential questions that can be asked when dealing with phonological questions, we consider especially the following three tasks as important, especially from the perspective of computable applications:\n",
    "\n",
    "1. (pseudo) word generation: the generation by whatever means of all *possible* sounds of a given language\n",
    "2. phonological rule induction: the induction (inference) of rules that trigger phonological changes in \n",
    "  - diachrony (from a source language to a target language), but also in \n",
    "  - synchrony (from a set of source forms to target forms within the same language)\n",
    "3. (pseudo) word prediction: the prediction of the possible outcome of a phonological process by which a source word form is converted, using a set of phonological rules, to a target form, again in\n",
    "  - diachrony (from an ancestral form to a descendant form), and again also\n",
    "  - synchrony (from a source form `[`or underlying form`]` to a target form `[`or surface form`]`)\n",
    "  \n",
    "These tasks do not cover all of phonology and phonological theory, but they capture many aspects that are dealt with in many schools of phonology, including generative syntax, optimality theory, and also classical historical linguistics.\n",
    "\n",
    "The task of (pseudo) word generation can be seen as the fundamental task of many approaches to phonology. If we assume that phonology defines phonotactic rules for a given language by which a set of symbols (the phonemes of the language) are transformed into valid forms of the language under investigation, which will be readily accepted as possible words of that very languages by its native speakers, can often be found in the phonological literature, even if it is rarely addressed by the authors explicitly. In traditional grammars of a language, for example, we often find a chapter on the phonotactics of a given language, where the authors describe the general syllable structure, using well-known formulas, such as `(C)(C)V(C)`, by which the general structure of possible syllables is described (with C referring to consonants and V referring to vowels). We can also find it in more theoretical accounts of phonology where scholars try to explain why a certain language does not allow for specific words. \n",
    "\n",
    "Word generation is also reflected in spoken language itself, as we can easily test when asking native speakers of a given language, whether some word that we just artificially created might form a possible word in their native tongue. We can find extreme limitations in the phonotactic systems of the languages of the world, where almost all possible words or syllables are valid. In Mandarin Chinese, for example, the amount of possible syllables (and syllables also correspond to morphemes) is limited to about 1600 different forms, of which, however, only about 1200 are readily realized in the language. If you ask a Chinese speaker whether *ká* is a valid word in Mandarin Chinese, most would probably answer that this is the case, even if no morpheme or word exists, in which you can find this form. But if you ask them, whether *tré* is a valid form, the would almost certainly deny it. \n",
    "\n",
    "In languages with a more complex phonology, like German, it is more difficult to determine which words are potentially valid German words. Since pseudo words play an important role for psycholinguistic experiments ([Keyleers and Brysbaert 2010](:bib:Keyleers2010)), the unbiased generatoin of pseudo-words in different languages plays a crucial practical role for the research. But also in classical phonology, scholars discuss to which degree words that are possible but not reflected in a given language occur. Software for pseudo-word generation exists (Keuleers and Brysbaert 2010), but most of the time, psycholinguists generate their pseudo-word candidates for their experiments manually, usually by shuffling syllables. Given that the inference and description and comparison of the phonotactic rules underlying a given language are a typical example for the fundamental research with which classical phonology should be concerned, it is surprising that the tasks of pseudo word generation and phonotactic description are barely addressed from within the same framework. We think that it is crucial, especially for computer-assisted accounts on historical linguistics (but also synchronic phonology), to emphasize the similarity between the tasks and to work on unified frameworks in which these task can be tackled both from a computational and a theoretical perspective.\n",
    "\n",
    "The two remaining tasks (rule inference and word prediction) are tightly connected with phonotactics, even if this may not seem to be obvious at the first sight. The similarity lies not only in the importance of the phonotactics of a given language to license a specific output (that needs to conform to the phonotactical rules of the target language), but more specifically in the dominance of the concept of phonetic or phonological context that plays a crucial role in both phonotactics and phonological change. From the phonotactic perspective, we can say that a language like German does not allow for voiced plosives (`[`b, d, g`]`) in the end of a word. From the perspective of phonological change, we can say that all voiced plosives in German words, if they occur in the end of the word, will be devoiced (`[`b, d, g`]` > `[`p, t, k`]` / `_`$). This rule results in morphonological alternations of voiced and voiceless plosives in the German plural (`[` h u n t `]` \"the dog\" vs. `[` h u n d ə `]` \"the dogs\").\n",
    "\n",
    "While rules and patterns such as final devoicing seem to be easy enough to handle in whatever framework, we have seen that the role that *context* (in whatever flavor it occurs) plays in synchronic and diachronic phonology can be quite complex, and that models of word generation that restrict the probability of finding an element in some position in a sound sequence only depend on the preceding segment will often hopelessly fail. We could handle final devoicing in German with these models by simply assuming a word generator that never ends a word in a voiced plosive, but we would fail in various other tasks, be they related to phonotactics, or historical sound change). \n",
    "\n",
    "That we cannot use simple models in which the probability of finding a given sound only depends on the preceding sound, is unfortunate, since these models, which are usually called +++first order+++ Markov models, are very well investigated, and a large number of tools is available that could be directly used to investigate language data in empirical frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2 Problems of current accounts in synchronic and diachronic phonology\n",
    "\n",
    "If we consider how phonology deals with the problems of word generation, rule inference, and word prediction, we can see several problems. These problems do not pertain to the theory of restrictions and rules (our tasks 1 and 2) *per se*, but rather in the procedures used by linguists to infer those restrictions and rules. The general problem is that the inference of restrictions and rules is a hard task: we always deal with a multitude of possibilities, but linguists have the ambition to find the *best* ones among these possibilities without being very concrete about what would qualify as an \"optimal\" restriction or rule to describe a given language. Linguists invoke various concepts, ranging from *cross-linguistic considerations* (rules recurring in many languages, and thus having predictive force beyond the description of one single language), *parsimony* (claiming that their account requires less assumptions than a given account by their colleagues), or *elegance* (as we can also often find it in mathematics, although there is no real criterion to judge the elegance of a given proposal). \n",
    "\n",
    "In general, the question of how to infer the rules is chiefly ignored in most approaches, probably also, because heuristic procedures that would confront scholars with all possible explanations for restrictions and rules would deprive scholars of the fun of trying to come up with new rules themselves. As a result, most accounts on phonology are restricted to the anecdotal, often only seemingly formal explanation of carefully ecclected phenomena, and discussions rarely focus on the data, but are restricted to carefully chose examples by which scholars try to make the point that their given system explains the few phenomena considered in a given study better than previous account. What we need, however, are rigorous empirical proposals that do no longer focus on exceptions and extremely challenging cases, but instead try to prove their usefulness based on a clear-cut set of principles that could ideally be applied to as many languages as possible. Ideally, each new account on phonological modeling should be tested on a large range of languages, and should be able to show that the tasks of word generation (1) and word prediction (3) are handled in a better way than previous accounts.\n",
    "\n",
    "What counts as *better* in this context is of course difficult to assess, but we think that a valid phonological theory should be based on parsimony with respect to a large sample of different languages. It should further be empirical and exhaustive, thus explaining not only the most challenging restrictions or exceptions, but ideally account for a very large number of words in the languages under consideration. \n",
    "\n",
    "What phonology needs, furthermore, is a clear-cut procedure for rule inference that, similar to recent attempts in mathematics to infer possible proofs computationally (see [automated proof checking](https://en.wikipedia.org/wiki/Automated_proof_checking) on Wikipedia), can also be automated, at least to some degree, in order to make sure that the initial inference of rules is not biased by the respective phonological school from which a proposal is derived. Another example for rule inference are recent applications in spreadsheet software, where tools like Microsoft's Excel try to infer the next value in a cell, after users have provided limited input for a certain range of cells (see [Gulwani 2011](:bib:Gulwani2011) for examples). As an example for the task of rule inference that software like Excel tries to accomplish here, consider sequences of numbers, such as 1, 3, 5, ? or 1, 2, 4, 7, 11, ?, and try to add the missing value. These tasks are similar to the task of rule inference in linguistics in so far as a couple of examples is given (and the possibility to further increase the number of examples, by further checking the data), and the taks is to infer how these examples can be explained with help of some rule or some set of rules. This comes very close to the task at hand in historical (and synchronic) phonology: based on either a set of words representing possible words in a language (our task number 1: word generation) or input-output examples where a couple of word pairs of which the second should be derived from the first is given, we try to infer the process that generates new words (task 1) or the rules by which we generate the output if we are given some input (task 2). Once we have inferred all rules, we can of course also generate the output of a given word for which we are only given the input (task 3).\n",
    "\n",
    "It is obvious that the tasks of rule inference has so far been only carried out manually in linguistics. Scholars rarely discuss heuristics describing how one could identify what relates an input to an output form, although it would, of course, be desirable to have either a computer-assisted or even a fully computer-based routine, not only because it would save us a lot of time, but also because it would help us to check the claims that a given inference of a set of rules is to be preferred over an alternative solution. \n",
    "\n",
    "The problem of inferring rules conditioning sound changes or restricting the possible words of a given language is that the phenomena we are dealing with are not entirely *simple*: what can follow a given sound in a language does not always only depend on what preceded it. Furthermore, we may have to accept that extra-phonological factors also have an impact on what we consider as possible words and what we neglect. \n",
    "\n",
    "As an example, consider four-consonant clusters in German, which can only occur after short vowels, like in *Herbst* \"autumn\", *färbst* \"(you) color\", *stirbst* \"you die\", or *kalbst* \"(you) give birth to a calf\". The sequences *rbst* and *lbst* are extreme clusters in the German languages which are really rare and -- apart from *Herbst* -- restricted to verbs, where *-st* marks the second person singular. If we create a pseudo word *nurbst* in German, it is without question that German speakers can pronounce this word, and it is also likely that they would accept it as a possible German word form. However, based on my intuition as a native speaker, I would predict that speaker would have a harder time licensing the word as \"German\", if it was proposed as *der Nurbst* as compared to *du nurbst*, simply, because the consonant combination is more often encountered as a complex morpheme (where speakers thus detect the verbal ending for the second person and interprete it accordingly) than as a single morpheme (where *Herbst* is our only exemplar in German). \n",
    "\n",
    "We would thus assume that restrictions and rules may -- at least to some degree -- have some *stochastic* component that cannot entirely be derived from the phonology alone. If this is the case, this clearly shows that we need models that allow for the inclusion of \"extra-phonological\" information, both for the task of word generation and of rule induction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2 Multiple Levels of Representation\n",
    "\n",
    "### 2.2.1 Syllable Templates in Phonological Description\n",
    "\n",
    "Given the complexity of the problem of rule induction, scholars often do not completely address these problems, but make use of certain shortcuts. One example for such a shortcut are templates for possible syllables in a given language. A typical representation of the Chinese syllable structure, for example, is the one given by [Sun (2006: 3](:bib:Sun2006):\n",
    "\n",
    "> That is, the canonical syllable structure in Chinese can be represented by the following: (C)(M)V(V/N). In other words, in standard Chinese a syllable may exist as a single vowel [...] or possibly consist of a string of phones [...] with a syllable structure like CMVV [...] or CMVN.\n",
    "\n",
    "Let us have a closer look at this description by inspecting some concrete examples of this structure. First of all, C in Sun's description represents one of the consonants in Standard Chinese. They are easy to describe as a list of IPA symbols, such as `[`p, pʰ, m, t, tʰ, n, k, kʰ, ŋ, s, ...`]`. V, of course, represents a list of vowels, such as `[`a, o, i, u, y, ...`]`. M represents a restricted class of glides, called *medial* in classical Chinese linguistics, pointing to sounds like `[`j, w, ...`]`. The symbol N represents *nasal consonants* in Sun's desription, thus pointing to the three elements `[`m, n, ŋ`]`. \n",
    "\n",
    "The problem with this description is that it is misleading if we try to use it to generate all possible syllables of the Chinese language, because it does not inform us about restrictions which are not licensed by the language. We can easily illustrate this by having a look at possible and impossible words in Standard Chinese:\n",
    "\n",
    "C | M | V | V/N | possible?\n",
    "--- | --- | --- | --- |: ---\n",
    "k | w | a | n | +\n",
    "k | j | a | ŋ | -\n",
    "Ø | j | o | u | +\n",
    "Ø | j | o | a | -\n",
    "n | Ø | a | Ø | +\n",
    "ŋ | Ø | a | Ø | -\n",
    "\n",
    "From the examples, we can see that the problem of the description results not only both the initial specification of the respective class which can appear in a given slot (consider the initial `[`ŋ`]`, which can't appear initially and could just be crossed off the list of initial consonants), but also from the combination of symbols (with `[`k`]` being impossible to combine with `[`j`]`, and `[`a`]` being invalid in coda-position). A satisfying model of the Chinese syllable should be able to create all *possible* sounds of the language, while not producing *impossible* ones. This means chiefly, that we *cannot* rely on simplifying templates when describing the language, as it is usually done in the syllable templates provided by scholars.\n",
    "\n",
    "\n",
    "If all we want is a model that describes all valid syllables of Standard Chinese, without carying for linguistic details or questions, we can easily circumvent this problem by eliciting *all* possible syllables of Chinese. This *is* possible, since the syllable structure of the language is so restricted that we only find about 400 possible syllables and even less than 1600 existing syllables when counting the tones. Even if unsatisfying, such a model would be completely accurate. It would consist of a simple lookup table, and whenever queried, whether a given string is a valid string of Standard Chinese, it would check the input string against the lookup table and return \"true\" if the input string could be found in the lookup table, and \"false\" in the opposite case. \n",
    "\n",
    "The disadvantage of such a model would, of course, be that it barely provides us with any new insights about Standard Chinese of phonology in general. Furthermore, if we compare the number of 1600 possible syllables with the observed syllables we find in languages like German, we can find that for a total of 6497 words in German (taken from a subset of the 2002 version of [Kluge's etymological dictionary](:bib:Kluge 2002), for which phonetic transcriptions in [Baayen et al. 1995](:bib:Baayen1995)) were available, is built from as many as 3305 different syllables (see below for the code example). Given that German has no tones, and that we simply counted this number by using LingPy's syllabification procedure, this illustrates that we cannot rely on eliciting all possible cases but should instead think about creating algorithms that infer the phonotactic rules of the languages of the world from empirical data. \n",
    "\n",
    "The following code lines show, how we can compute the number of different syllables in a given dataset. Since LingPy's syllabification function is a bit rough, using only prosody as a proxy, the results need to be taken with care, though, as it is quite likely that we undercount the number of different syllables, since the algorithm artificially breaks valid syllables in German that do not account to the principle of increasing and decreasing sonority around the syllable nucleus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3305 syllables taken from  6497 words in total (2.22 syllables per word).\n"
     ]
    }
   ],
   "source": [
    "from lingpy import *\n",
    "from lingpy.sequence.sound_classes import syllabify\n",
    "rc(morpheme_separator=\"+\")\n",
    "data = [x[2] for x in csv2list('../data/S11-kluge-extracted.tsv', strip_lines=False)[1:] if x[2].strip()]\n",
    "syls = set()\n",
    "syllen = [0]\n",
    "for word in data:\n",
    "    for syl in syllabify(word.split(), output=\"nested\"):\n",
    "        syls.add(' '.join(syl))\n",
    "        syllen[-1] += 1\n",
    "    syllen += [0]\n",
    "print('{0:5} syllables taken from {1:5} words in total ({2:.2f} syllables per word).'.format(\n",
    "    len(syls), len(data), sum(syllen) / len(syllen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "Even if we agree that we should use models that *generate* all possible words of a given language, this does not necessarily mean that we need to rely on the classical idea that a language has a simple sound inventory consisting of sound segments like consonants, vowels, and tones, which the model then concatenates in order to form possible words. The problem of such a naive model is that it ignores the fact that many if not all languages have strong distributional restrictions regarding the positions where certain sounds can appear. The sound `[`ŋ`]`, for example, cannot appear in initial position in German, and the same also holds for Standard Chinese. We can easily address this problem by applying a fuzzy clustering of the sounds of a given language which informs us about these very rough contextual restrictions. We could thus differentiate between possible initial and final consonants. In our example of Standard Chinese, this would mean that `[`ŋ`]` would not appear in the list of initial consonants, but only in the list of final consonants, while `[`n`]` would appear in both lists.  \n",
    "\n",
    "We could even pursue a more radical strategy by treating combinations of initial and medial in SEA languages as a single unit of analysis. This would blow up the number of sounds in our system, but it would help us to go through the pain of inferring context rules for all possible combinations of initial and medial sounds. In fact, we could even argue that syllables like `[`kwan`]` in Chinese could likewise be representad as `[`kw a n `]`, and in an even more radical notion, we could do the same for liquid clusters in German (`[`kr, tr, pr, ...`]`). Inferring rules would become simpler, if we increase the number of sounds in our supposed system of German, although linguists would probably intuitively oppose to this idea.\n",
    "\n",
    "No matter how we model the phonotactics of a given language in the end, we will always have to find the right trade-off between the size of the different symbols we use and the complexity (or number) of rules that we use to combine the symbols to form strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.2 From Syllable Templates to Sequential Representations\n",
    "\n",
    "Syllable templates like the one by Sun (2006) mentioned above have the advantage of providing an easy shortcut to approximate the phonology of a language. Their disadvantage is, as we have seen, that they do not really reflect possible and impossible words truthfully. An important aspect that we can employ, however, is the use of specific *sound classes* to indicate in which slots of a syllable which sounds can occur. If we combine a set of possible syllable templates with a set of sound classes (where one and the same sound can potentially be assigned to the same class), we can quite quickly and in a very straightforward manner create a simple model that generates possible words for very different types of languages. \n",
    "\n",
    "The following model, for example, creates languages that remind a bit of the typical tone-less languages of South-East Asia, in which monosyllabic morphemes are the predominant pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1 lemu\n",
      "    2 pluttrar\n",
      "    3 kebranke\n",
      "    4 kotlep\n",
      "    5 la\n",
      "    6 rodep\n",
      "    7 mu\n",
      "    8 pikbet\n",
      "    9 do\n",
      "   10 mu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "syllables = [\"CV\", \"CVc\", \"CVc\", \"CRVc\", \"RV\", \"NV\"]\n",
    "sounds = {\n",
    "    \"V\": [\"a\", \"i\", \"e\", \"o\", \"u\"],\n",
    "    \"C\": [\"ts\", \"p\", \"t\", \"k\", \"b\", \"d\", \"g\"],\n",
    "    \"R\": [\"l\", \"r\"],\n",
    "    \"N\": [\"m\", \"n\"],\n",
    "    \"c\": [\"p\", \"t\", \"k\", \"r\", \"l\", \"n\", \"m\", \"ŋ\"]\n",
    "}\n",
    "def make_word():\n",
    "    length = random.randint(1,3)\n",
    "    outs = []\n",
    "    for i in range(length):\n",
    "        syl = random.choice(syllables)\n",
    "        out = ''\n",
    "        for elm in syl:\n",
    "            out += random.choice(sounds[elm])\n",
    "        outs += [out]\n",
    "    return ''.join(outs)\n",
    "for i in range(10):\n",
    "    print('{0:5} {1}'.format(i+1, make_word()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The disadvantage of such a model, however, is that it has essentially two parts: one that creates the syllable template, and one that fills the template slots with the respective sound classes. The problem of the two-layer structure is that the model creating such a language is:\n",
    "\n",
    "1. difficult to infer (we have to conclude on two unknown structures: the syllable template and the classes for each of the sounds in the language), and \n",
    "2. difficult to evaluate (given a string, it is not easy to say whether it could be produced by our model, since we'll need to write one algorithm that aligns the word to a syllable template and then checks whether sound slots are properly filled).\n",
    "\n",
    "The problem of these models is that they are no longer *sequential*. While simple sequential models can be easily handled with help of a regular grammar that produces string after string based on an underlying network of transition probabilities, our two-layer model has an underlying hierarchy.\n",
    "\n",
    "We can, however, find a very simple way to approximate the two-layered structure as a sequence by representing the 16 sounds no longer as single units, but instead representing them as bigrams, one consisting of their original value (the sound in IPA format), and one consisting of their sound-class value as we defined them in our initial dictionary above. This will create more \"different\" sounds than we had in our hierarchical presentation, but it has the advantage of providing us with a means to handle the process of word generation in a simple transition graph (a Markov model, see part +++ below). We can create this graph directly from our initial model, and all we have to add is a tweak that makes clear the model terminates often enough. But with this sequential representation of our data produced by a hierarchy, we use a few more lines of code, yet we have the advantage of dealing with a model that can be easily understood and applied, and also be learned from the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition graph has 35 nodes\n",
      "    1 lula\n",
      "    2 natlulit\n",
      "    3 glip\n",
      "    4 piboket\n",
      "    5 dadrupgaroguge\n",
      "    6 ga\n",
      "    7 lik\n",
      "    8 bit\n",
      "    9 ti\n",
      "   10 do\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from lingpy.sequence.sound_classes import bigrams\n",
    "\n",
    "TG = nx.DiGraph() # transition graph\n",
    "syllables = [\"CV\", \"CVc\", \"CVc\", \"CRVc\", \"RV\", \"NV\"]\n",
    "sounds = {\n",
    "    \"V\": [\"a\", \"i\", \"e\", \"o\", \"u\"],\n",
    "    \"C\": [\"ts\", \"p\", \"t\", \"k\", \"b\", \"d\", \"g\"],\n",
    "    \"R\": [\"l\", \"r\"],\n",
    "    \"N\": [\"m\", \"n\"],\n",
    "    \"c\": [\"p\", \"t\", \"k\", \"r\", \"l\", \"n\", \"m\", \"ŋ\"]\n",
    "}\n",
    "for syl in syllables:\n",
    "    for elmA, elmB in bigrams(syl):\n",
    "        for soundA in sounds.get(elmA, [elmA]):\n",
    "            for soundB in sounds.get(elmB, [elmB]):\n",
    "                TG.add_edge(soundA+'/'+elmA, soundB+'/'+elmB)\n",
    "            # add start symbols instead of terminal symbol to make sure we can produce longer words as well\n",
    "            if elmB == '$':\n",
    "                for soundC in sounds['R']:\n",
    "                    TG.add_edge(soundA+'/'+elmA, soundC+'/R')\n",
    "                    TG.add_edge(soundA+'/'+elmA, soundC+'/$')\n",
    "                for soundC in sounds['C']:\n",
    "                    TG.add_edge(soundA+'/'+elmA, soundC+'/C')\n",
    "                    TG.add_edge(soundA+'/'+elmA, soundC+'/$')\n",
    "                for soundC in sounds['R']:\n",
    "                    TG.add_edge(soundA+'/'+elmA, soundC+'/R')\n",
    "                    TG.add_edge(soundA+'/'+elmA, soundC+'/$')\n",
    "print('Transition graph has {0} nodes'.format(len(TG)))\n",
    "nx.write_gml(TG, 'transition_graph')\n",
    "\n",
    "def make_word_from_graph():\n",
    "    out = ''\n",
    "    neighbors = list(TG['#/#'])\n",
    "    while True:\n",
    "        next_elm = random.choice(neighbors)\n",
    "        if next_elm[-1] != '$':\n",
    "            out += next_elm.split('/')[0]\n",
    "            neighbors = list(TG[next_elm])\n",
    "        else:\n",
    "            break\n",
    "    return ''.join(out)\n",
    "for i in range(10):\n",
    "    print('{0:5} {1}'.format(i+1, make_word_from_graph()))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we will see below, the general idea of this sequential representation of an inherently hierarchical model is to increase the size of the alphabet by distinguishing sounds not only with respect to their pure IPA string quality, but also with respect to the context in which they can occur. Contex is hereby broadly defined. In our example above, it refers to the sound symbols that we used in our syllable template, but we can think of many more ways to represent context information as part of the segment of a given sequence. \n",
    "\n",
    "In LingPy, for example, sequences can be represented as tuples of sound classes and prosodic strings. This is the general structure employed by the LexStat algorithm ([List 2014](:bib:List2014d), which we can easily illustrate by loading a file with the LexStat class of LingPy. If LexStat loads a dataset, it automatically adds the information needed to create the different layers of the sequential representation (also called the \"tiers\"). These are, among others: the sound classes (\"CLASSES\"), the prosodic string (\"PROSTRING\"), and the original sequence in tokenized form: \"TOKENS\". The prosodic string itself is derived from a vector representing the sonority of the segments (\"SONARS\"), but also the gaph-weights, which indicate, how easily a given position can be gapped (\"WEIGHTS\").\n",
    "\n",
    "The following code piece contrasts the different pieces of information for the Icelandic counterpart of \"bark (of tree)\" in our small Germanic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes              P    \tU    \tR    \tK    \tY    \tR    \n",
      "sonars                1.00\t 7.00\t 5.00\t 1.00\t 7.00\t 5.00\n",
      "prostrings           A    \tX    \tM    \tB    \tY    \tN    \n",
      "numbers              5.P.C\t5.U.V\t5.R.c\t5.K.C\t5.Y.V\t5.R.c\n",
      "weights                2.0\t  1.5\t  1.1\t 1.75\t  1.3\t  0.8\n",
      "tokens               b    \tœ    \tr    \tk    \tʏ    \tr    \n"
     ]
    }
   ],
   "source": [
    "lex = LexStat('../data/S10-GER.tsv')\n",
    "for h in ['classes', 'sonars', 'prostrings', 'numbers', 'weights', 'tokens']:\n",
    "    print('{0:20}'.format(h), '\\t'.join(['{0:5}'.format(x) if not str(x).isdigit() else '{0:5.2f}'.format(x) for x in lex[19, h]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.3 Enhanced Modeling of Sound Sequences with Help of Multi-Tiered Sequence Models\n",
    "\n",
    "We can easily expand on the idea of representing more than just the sound segments when representing a word from in linguistics, if we only accept the idea of \n",
    "\n",
    "1. taking the sound segments as the smallest (atomic) unit of representation, and\n",
    "2. forcing all values for all levels to be aligned with the smallest representation (while accepting to repeat information if needed).\n",
    "\n",
    "Let us consider a very simple example for such a *multi-tiered sequence representation*, by parsing the word Bulgarian *jabəlka* \"apple\" with different representations provided by LingPy. We start with the classical representations of sound classes (using simple \"consonant-vowel\" models), but we further expand our model by checking not only the sound class of a given sound, but also which sound class follows or precedes a given sound:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What these structures provide is a pool of different possibly important contexts for either word generation or rule inference. We can easily adjust the contexts, by adding additional information that goes beyond what follows left and right of a given segment. For example, when dealing with tonal sequences, we can just assign the tone to each sound segment and no longer annotate the tone as a separate unit:\n",
    "\n",
    "form | in. | nuc. | fin. \n",
    "--- | --- | --- | --- \n",
    "piŋ⁵⁵ | p / ⁵⁵ | i / ⁵⁵ | ŋ / ⁵⁵ \n",
    "pin³⁵ | p / ³⁵ | i / ³⁵ | n / ³⁵\n",
    "\n",
    "While this may seem strange at first sight, it makes a lot of sense, also as a heuristic procedure, since we know well that tone may often correlate with voicing and devoicing in the history of the Chinese dialects. If we model Middle Chinese source forms as multi-tiers in which the tone information is given as part of the information on the initial, we can immediately see that the Middle Chinese \"flat\" tone (*píng*, P) leads to a different reflex in some dialects than the three other tones (*shǎng*, *qù*, *rù*, SQR), as shown in the following figure (List under review):\n",
    "\n",
    "![img](img/s11-fig1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What this essentially shows is that we can use the idea of using multi-layered, multi-tiered sequence representations to generate a pool of potential hypotheses from which we can then algorithmically or manually pick those that account for a given output. Multi-tiered sequence representation is thus, as we will see in the following section, a way to generate hypotheses to derive or induce rules, addressing our task 2, mentioned above. Furthermore, as multi-tiers preserve the sequential representation level, we can also use tools like Markov models to handle questions of word generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 Computational Perspective on Tier Representations (Tiago Tresoldi)\n",
    "\n",
    "As in biology, linguistic data and relationships are commonly represented and visualized with one of three fundamental structures: the sequence, for sequential data, the tree, for hierarchical non-sequential data, and the network, for non-hierachical non-sequential data. Typical examples are n-grams, syntax tree, and colexification networks.\n",
    "\n",
    "![ngrams](img/ngrams.jpg)\n",
    "\n",
    "**Ngrams**\n",
    "\n",
    "![syntax tree](img/syntaxtree.jpg)\n",
    "\n",
    "**Syntax Tree**\n",
    "\n",
    "![colex](img/colex.jpg)\n",
    "\n",
    "**Colexification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The sequential representation is by far the most common one for representing sound sequences, both for easiness of using it, for the example of alphabets, and for the fact that, even though suprasegmental features might be at play, sound sequences are themselves sequential and temporally delimited. As such, most mathematical models for representing sequences of speech sounds are some kind of sequence (defined as an \"enumerated collection of objects in which repetitions are allowed and with no overlapping between the members\"), with member as samples from a random distribution. It is important to remember that, in this mathematical sense, \"random\" does not necessarily means \"without a specific pattern\" or \"unpredictable\" (as the sequence of sounds is usually quite predictable, for example due to language inventory and phonotactics), but just that the sequence is composed of different elements that follow a probability distribution.\n",
    "\n",
    "The most common way of modelling such sequentes is with a Markov model. While there are many different structures and approaches that qualify as \"Markovian\" (such as Markov chains, Hidden Markov Models, etc.), we can simply think of a Markov model simply as a model describing a sequence of possible events (the \"elements\") in which the probability of each event depends only on the state attained in the previous event. For this reason, Markov models are \"memoryless\", in the sense that nothing informs the transition between states with the exception of the current one and, more important, that the probability for a transition is the same no matter its position in the sequence and the number of previous transitions; however, it is necessary to remember that the current state is not necessarily composed of a single element, as we can, for example, model a language using bigrams and trigrams (that is, preceding sequences of two or three characters that inform on the probability of the next character).\n",
    "\n",
    "For a more concrete example of Markov models, in English, if we have no other information about the context of occurence, after a sound /ð/ (represented in writing by \"TH\") we are right to expect the sound /iː/ (\"E\") as the most probable, given that the article \"THE\" is the most common word in the language and that the /ðiː/ sequence has a high overall frequency. We can, in fact, easily train a model on such premises. If we consider a context composed only of one sound (a \"monogram\", even though, as we just mentioned, larger, combined, and other alternatives are possible), we can easily come up with a matrix of probabilities of transition from state to state (i.e., from sound to sound) such as the following one by counting the number of observations of a sound S1 followed by a sound S2.\n",
    "\n",
    "| S1 \\ S2    | aɪ | aʊ    | b    | d    | dʒ   | (...) |ʒ    | θ |\n",
    "|-----|----|-------|------|------|------|-------|------|---|\n",
    "| aɪ | 0.03 | 0.04 | 2.53 | 9.18 | 0.55 |-------| 0.00 | 0.10 |\n",
    "| aʊ | 0.03 | 0.00 | 2.01 | 4.68 | 0.25 |-------| 0.00 | 2.20 |\n",
    "| b | 2.24 | 1.20 | 0.00 | 0.51 | 0.15 |-------| 0.00 | 0.01 |\n",
    "| d | 2.50 | 1.09 | 0.79 | 0.02 | 0.02 |-------| 0.00 | 0.05 |\n",
    "| dʒ | 1.18 | 0.14 | 0.04 | 2.49 | 0.00 |-------| 0.00 | 0.00 |\n",
    "| (...) |\n",
    "| ʒ | 0.55 | 1.11 | 0.00 | 2.21 | 0.00 |-------| 0.00 | 0.00 |\n",
    "| θ | 1.77 | 0.71 | 1.15 | 0.62 | 0.09 |-------| 0.00 | 0.00 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While a model such as this is reasonably effective given its simplicity, it fails to support more advanced modelings such as those described above. In terms of patterns of sounds, for example, while the probability distributions it encapsulates could be used as a first step for generalizing observations, there are limits in terms of context and not much can be done without a human intervention in each single stage. In terms of phonetic representation, it cannot fully account for suprasegmental features such as tones (i.e., the contrastive elements that cannot be analyzed as distinct segments, but which belong to a subgroup of them, non necessarily contiguous, as in the case of vowel harmony, and not even necessarily following the same segment boundaries). Expanding the context and the number of observations to capture suprasegmental features ends up modelling each combination of features on their own, losing any generalization; in fact, for other usages such as pseudo-word generation, not only the context is too limited to capture complex patterns as well as medium- and long-distance relationships, it is also unable to encode information that is external to the sum of individual elements (for example, if the pseudo-word is supposed to be a verb or a noun). \n",
    "\n",
    "Some of these problems can be solved or partially remediated with more complex approaches, such as higher order n-grams (i.e., more context) or combined information, but we cannot surpass the limit that each element can only encode one item of information and that our system might be more than the sum of its elements and their relationships. This is particularly valid in linguistic research, where many of the possibilities and hypotheses are weighted in terms of typological knowledge that is not encoded in what is being observed.\n",
    "\n",
    "One possible objection to this is that information such as IPA graphemes is by no way atomic: even without considering possible suprasegmental information or more context, a graphame such as /b/ already carries many levels, or \"tiers\", of information, such as its manner of articulation (\"stop\"), its place of articulation (\"bilabial\"), its voiceness (\"voiced\"), and so on, including \"negative information\" such as the impossibility of it being a syllable nucleus (in most languages). Additional information can be obtained from the word where the sound is realized, such as the stress of its syllable, and even from the system where it is produced, such as the frequency of the word under analysis, its age, the donor language in case of borrowings, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our proposed solution is to consider parallel, multilayered and conceptually linked sequences, which are partly analogous to some solutions with marginal adoption in stochastic methods such as Layered Hidden Markov models. In our proposal, a number of \"tiers\" (levels) can be expressed in its relationship to a given sequence (i.e., word). While the most obvious ones are distinctive features, suprasegmental information and extra lexical information, such as we just described, this can potentially accomodate even the relationship between two or more words, such as cognates. One reduced example is presented here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "| Tier             | Description            | Alignment           |\n",
    "|------------------|------------------------|---------------------|\n",
    "| SOURCE           | source sounds          | s | w | e  | r | d  |\n",
    "| CV  ?X           | previous sound C or V  | ∅ | C | C  | V | C  |\n",
    "| CV  X?           | following sound C or V | C | C | C  | C | ∅  |\n",
    "| SOUND CLASS  ?X  | previous sound class   | ∅ | S | W  | V | R  |\n",
    "| SOUND CLASS  X?  | following sound class  | W | V | R  | T | ∅  |\n",
    "| STRESS           | stress in source       | 1 | 1 | 1  | 1 | 1  |\n",
    "| TARGET           | target sounds          | ʃ | v | e: | r | t  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While a huge amount of tiers is not feasible for human inspection and manipulation, tiers can be easily pre-processed by algorithms for finding relationships that would take many hours of work of manual inspection; in particular, automatic methods should be able to easily deal and remove the many layers of information that are either strongly correlated or plain redudant (for example, the fact that all vowels are voiced). One preliminary example of this is shown below, where we build an aligned dataset of reconstructed Proto-Germanic and German words and test, with a decision tree, for the features that are significant in terms of prediction of the initial sound in a German word given the initial sound in the corresponding Proto-Germanic word. In this toy example, we are considering all features from \"source\" (Proto-Germanic) but only two features, \"stop\" and \"voiced\", from \"target\" (German).\n",
    "\n",
    "![dt](img/dt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While this is a toy example with only 101 observations and a single tree (we should really run it multiple times, having a \"forest\" that allows us to balance the results of each tree), it is still able to capture some of the advantages of a multitier system. In particular, the algorithm identified \"alveolar\" as the most informative tier in terms of relationship between the \"source\" (Proto-Germanic) and the \"target\" (German) tier. The very first decision is based on the initial phoneme in Proto-Germanic not being an alveolar, which when true (left branch) leads to a population where all sounds are stops (70 stop vs. 0 non-stops), as our dataset only has alveolar fricatives; among the stops, the only decision we need to predict the target is whether the source sound is voiced or not, with 13 voiceless plosives and 57 voiced ones. \n",
    "\n",
    "The decision tree captured the most common correspondences in terms of manner of articulation and voiceness between initials in Proto-Germanic and German:\n",
    "\n",
    "* voiced non-alveolars were kept as such (first leaf, e.g., boːk/buːk)\n",
    "\n",
    "| Tier           | #1 | #2 | #3 |\n",
    "|----------------|----|----|----|\n",
    "| Proto-Germanic |  b | oː |  k |\n",
    "| alveolar       |  0 |  0 |  0 |\n",
    "| voiced         |  1 | 1  |  0 |\n",
    "| front +2       |  0 |  0 |  0 |\n",
    "| German         |  b | uː |  x |\n",
    "\n",
    "\n",
    "* voiceless non-alveolars became voiced, usually with a change in manner of articulation which is not considered here (second leaf, e.g. θuːrst/dʊrst)\n",
    "\n",
    "| Tier           | #1 | #2 | #3 | #4 | #5 |\n",
    "|----------------|----|----|----|----|----|\n",
    "| Proto-Germanic |  θ | uː |  r |  s |  t |\n",
    "| alveolar       | 0  | 0  |  1 | 1  | 1  |\n",
    "| voiced         | 0  | 1  | 1  | 0  | 0  |\n",
    "| front +2       | 0  | 0  | 0  | 0  | 0  |\n",
    "| German         |  d |  ʊ |  r |  s |  t |\n",
    "\n",
    "\n",
    "* voiced alveolars were kept as such (third leaf, e.g., liːna/lainə)\n",
    "\n",
    "| Tier           | #1 | #2 | #3 | #4 |\n",
    "|----------------|----|----|----|----|\n",
    "| Proto-Germanic |  l | iː |  n |  a |\n",
    "| alveolar       | 1  | 0  |  0 | 0  |\n",
    "| voiced         | 1  | 1  | 1  | 1  |\n",
    "| front +2       | 0  | 0  | 0  | 0  |\n",
    "| German         |  l |  ai |  n |  ə |\n",
    "\n",
    "\n",
    "* voiceless alveolars where kept as such, but usually with a change in manner of articulation, if the third sound was a vowel (fourth leaf, e.g., swesteːr/ʃvɛstər)\n",
    "\n",
    "| Tier           | #1 | #2 | #3 | #4 | #5 | #6 | #7 |\n",
    "|----------------|----|----|----|----|----|----|----|\n",
    "| Proto-Germanic |  s | w  |  e |  s |  t | eː | r  |\n",
    "| alveolar       | 1  | 0  |  0 | 1  | 0  | 0  |  1 |\n",
    "| voiced         | 0  | 1  | 1  | 0  | 0  | 1  |  1 |\n",
    "| front +2       | 1  | 0  | 0  | 1  | 0  | 0  | 0  |\n",
    "| German         |  ʃ |  v |  ɛ |  s |  t | ə  | r  |\n",
    "\n",
    "* voiceless alveolars became voiced if the following sound is a vowel (fifth leaf, e.g., setjan/zɪtsən).\n",
    "\n",
    "| Tier           | #1 | #2 | #3  | #4 | #5 | #6 |\n",
    "|----------------|----|----|-----|----|----|----|\n",
    "| Proto-Germanic |  s | e  |  t  |  j |  a |  n |\n",
    "| alveolar       | 1  | 0  |  0  | 0  | 0  | 0  |\n",
    "| voiced         | 0  | 1  | 0   | 1  | 1  | 1  | \n",
    "| front +2       | 0  | 0  | 0   | 0  | 0  | 0  |\n",
    "| German         |  z |  ɪ |  ts | - |  ə | n  |\n",
    "\n",
    "\n",
    "While reading and interpreting the results of a decision tree is not easy, and even harder when random forests are involved, these results are a good demonstration of how such system would work and how it is able to capture patterns of correspondence given a set of examples. It is also theoretically possible to map such decisions to a standard A > B / C notation as used in historical linguistics.\n",
    "\n",
    "A couple of notes before ending. First, in terms of having to generate a single tree. While the first decision was based in feature \"alveolar\", due to the fact of all initial fricatives in our dataset being alveolar, most linguists would probably opt for the statistically equal rule based on feature \"manner\", as, without any additional information, sound correspondences are more likely to be related to manner than place of articulation. Multiple runs of the algorithm (\"the random forest\") would probably capture both rules (manner and place of articulation) with the same probability, which is a good indication that, given *exclusively* the encoded information, they are equally informative. The preference for manner of articulation is an expert bias, not necessarily false, that should either be accounted as such or encoded with enough information so that the algorithm is able to capture it.\n",
    "\n",
    "A second note is in terms of tree pruning, i.e., reducing the size of the model. While we are presenting the entire tree, from the population of the fourth and last level (the right-most decision, at the bottom), we can see that an entire rule is estipulated to account for a single case of voiced stop. Rules such as this allow to quickly eyeball potential errors and outliers in our dataset (such as typos or borrowings), but we can also estipulate a cutoff in terms of decision depth, so as to capture the most informative rules (i.e., the ones that explain most of the observed data).\n",
    "\n",
    "A final reminder is that we are not forced to compare only \"source\" and \"target\", and, in fact, the system allows us to compare any group of tiers, including multiple words. For example, we could encode other Germanic languages and try to capture unexplained transitions in one language by tier observed in a sister language, thus considering information that is lost in the first one but can be accounted for by its cognates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
